{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network hybrid recommendation system on Google Analytics data model and training\n",
    "\n",
    "This notebook demonstrates how to implement a hybrid recommendation system using a neural network to combine content-based and collaborative filtering recommendation models using Google Analytics data. We are going to use the learned user embeddings from [wals.ipynb](../wals.ipynb) and combine that with our previous content-based features from [content_based_using_neural_networks.ipynb](../content_based_using_neural_networks.ipynb)\n",
    "\n",
    "Now that we have our data preprocessed from BigQuery and Cloud Dataflow, we can build our neural network hybrid recommendation model to our preprocessed data. Then we can train locally to make sure everything works and then use the power of Google Cloud ML Engine to scale it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use TensorFlow Hub to use trained text embeddings, so let's first pip install that and reset our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub==0.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/22/64f246ef80e64b1a13b2f463cefa44f397a51c49a303294f5f3d04ac39ac/tensorflow_hub-0.1.1-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub==0.1.1) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub==0.1.1) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub==0.1.1) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.4.0->tensorflow_hub==0.1.1) (40.2.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub==\"0.1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reset the notebook's session kernel! Since we're no longer using Cloud Dataflow, we'll be using the python3 kernel from here on out so don't forget to change the kernel if it's still python2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import helpful libraries and setup our project, bucket, and region\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-0307cfdd60478525' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'qwiklabs-gcp-0307cfdd60478525' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-east1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-0307cfdd60478525/...\n",
      "ServiceException: 409 Bucket qwiklabs-gcp-0307cfdd60478525 already exists.\n",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/eval.csv-00000-of-00001 [Content-Type=text/plain]...\n",
      "/ [0 files][    0.0 B/ 11.4 MiB]                                                \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/tmp/staging/preprocess-hybrid-recommendation-features-181217-164834.1545065316.946936/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 13.6 MiB]                                                \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/tmp/staging/preprocess-hybrid-recommendation-features-181217-164834.1545065316.946936/dataflow_python_sdk.tar [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 16.0 MiB]                                                \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/tmp/staging/preprocess-hybrid-recommendation-features-181217-164834.1545065316.946936/pipeline.pb [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 16.1 MiB]                                                \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00000-of-00004 [Content-Type=text/plain]...\n",
      "/ [0 files][    0.0 B/ 18.0 MiB]                                                \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00001-of-00004 [Content-Type=text/plain]...\n",
      "/ [1/21 files][ 41.3 KiB/128.8 MiB]   0% Done                                   \r",
      "/ [1/21 files][ 41.3 KiB/128.8 MiB]   0% Done                                   \r",
      "/ [2/21 files][  2.0 MiB/128.8 MiB]   1% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00002-of-00004 [Content-Type=text/plain]...\n",
      "/ [2/21 files][  2.0 MiB/128.8 MiB]   1% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00003-of-00004 [Content-Type=text/plain]...\n",
      "/ [3/21 files][  4.3 MiB/128.8 MiB]   3% Done                                   \r",
      "/ [3/21 files][  4.3 MiB/128.8 MiB]   3% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/author_vocab_count.txt-00000-of-00001 [Content-Type=text/plain]...\n",
      "/ [4/21 files][  6.7 MiB/128.8 MiB]   5% Done                                   \r",
      "/ [4/21 files][  6.7 MiB/128.8 MiB]   5% Done                                   \r",
      "-\r",
      "- [5/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/category_vocab_count.txt-00000-of-00001 [Content-Type=text/plain]...\n",
      "- [5/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "- [6/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/content_id_vocab_count.txt-00000-of-00001 [Content-Type=text/plain]...\n",
      "- [6/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/months_since_epoch_mean.txt-00000-of-00001 [Content-Type=text/plain]...\n",
      "- [7/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "- [7/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/tmp/staging/preprocess-hybrid-recommendation-vocab-counts-181217-170736.1545066458.457255/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl [Content-Type=application/octet-stream]...\n",
      "- [8/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "- [8/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "- [9/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/tmp/staging/preprocess-hybrid-recommendation-vocab-counts-181217-170736.1545066458.457255/dataflow_python_sdk.tar [Content-Type=application/octet-stream]...\n",
      "- [9/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/tmp/staging/preprocess-hybrid-recommendation-vocab-counts-181217-170736.1545066458.457255/pipeline.pb [Content-Type=application/octet-stream]...\n",
      "\\\r",
      "\\ [10/21 files][ 39.3 MiB/128.8 MiB]  30% Done                                  \r",
      "\\ [10/21 files][ 39.3 MiB/128.8 MiB]  30% Done                                  \r",
      "\\ [11/21 files][ 89.9 MiB/128.8 MiB]  69% Done                                  \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/author_vocab.txt-00000-of-00001 [Content-Type=text/plain]...\n",
      "\\ [11/21 files][ 89.9 MiB/128.8 MiB]  69% Done                                  \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001 [Content-Type=text/plain]...\n",
      "\\ [12/21 files][ 92.1 MiB/128.8 MiB]  71% Done                                  \r",
      "\\ [12/21 files][ 92.1 MiB/128.8 MiB]  71% Done                                  \r",
      "\\ [13/21 files][121.5 MiB/128.8 MiB]  94% Done                                  \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/content_id_vocab.txt-00000-of-00001 [Content-Type=text/plain]...\n",
      "\\ [13/21 files][121.5 MiB/128.8 MiB]  94% Done                                  \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/tmp/staging/preprocess-hybrid-recommendation-vocab-lists-181217-170024.1545066026.063994/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl [Content-Type=application/octet-stream]...\n",
      "\\ [14/21 files][123.9 MiB/128.8 MiB]  96% Done                                  \r",
      "\\ [14/21 files][123.9 MiB/128.8 MiB]  96% Done                                  \r",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/tmp/staging/preprocess-hybrid-recommendation-vocab-lists-181217-170024.1545066026.063994/dataflow_python_sdk.tar [Content-Type=application/octet-stream]...\n",
      "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/tmp/staging/preprocess-hybrid-recommendation-vocab-lists-181217-170024.1545066026.063994/pipeline.pb [Content-Type=application/octet-stream]...\n",
      "\\ [15/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r",
      "\\ [15/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r",
      "\\ [16/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r",
      "\\ [16/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r",
      "\\ [17/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r",
      "\\ [18/21 files][124.1 MiB/128.8 MiB]  96% Done                                  \r",
      "\\ [19/21 files][126.4 MiB/128.8 MiB]  98% Done                                  \r",
      "\\ [20/21 files][128.8 MiB/128.8 MiB]  99% Done                                  \r",
      "|\r",
      "| [21/21 files][128.8 MiB/128.8 MiB] 100% Done                                  \r\n",
      "Operation completed over 21 objects/128.8 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/hybrid_recommendation/preproc; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "  # copy canonical set of preprocessed files if you didn't do preprocessing notebook\n",
    "  gsutil -m cp -R gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create hybrid recommendation system model using TensorFlow </h2>\n",
    "\n",
    "Now that we've created our training and evaluation input files as well as our categorical feature vocabulary files, we can create our TensorFlow hybrid recommendation system model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get some of our aggregate information that we will use in the model from some of our preprocessed files we saved in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_content_ids = 15634\n"
     ]
    }
   ],
   "source": [
    "# Get number of content ids from text file in Google Cloud Storage\n",
    "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/content_id_vocab_count.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
    "  number_of_content_ids = int([x for x in ifp][0])\n",
    "print(\"number_of_content_ids = {}\".format(number_of_content_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_categories = 3\n"
     ]
    }
   ],
   "source": [
    "# Get number of categories from text file in Google Cloud Storage\n",
    "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/category_vocab_count.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
    "  number_of_categories = int([x for x in ifp][0])\n",
    "print(\"number_of_categories = {}\".format(number_of_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_authors = 1103\n"
     ]
    }
   ],
   "source": [
    "# Get number of authors from text file in Google Cloud Storage\n",
    "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/author_vocab_count.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
    "  number_of_authors = int([x for x in ifp][0])\n",
    "print(\"number_of_authors = {}\".format(number_of_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_months_since_epoch = 573.60733908\n"
     ]
    }
   ],
   "source": [
    "# Get mean months since epoch from text file in Google Cloud Storage\n",
    "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/months_since_epoch_mean.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
    "  mean_months_since_epoch = float([x for x in ifp][0])\n",
    "print(\"mean_months_since_epoch = {}\".format(mean_months_since_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "NON_FACTOR_COLUMNS = 'next_content_id,visitor_id,content_id,category,title,author,months_since_epoch'.split(',')\n",
    "FACTOR_COLUMNS = [\"user_factor_{}\".format(i) for i in range(10)] + [\"item_factor_{}\".format(i) for i in range(10)]\n",
    "CSV_COLUMNS = NON_FACTOR_COLUMNS + FACTOR_COLUMNS\n",
    "LABEL_COLUMN = 'next_content_id'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "NON_FACTOR_DEFAULTS = [[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[mean_months_since_epoch]]\n",
    "FACTOR_DEFAULTS = [[0.0] for i in range(10)] + [[0.0] for i in range(10)] # user and item\n",
    "DEFAULTS = NON_FACTOR_DEFAULTS + FACTOR_DEFAULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input function for training and evaluation to read from our preprocessed CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input function for train and eval\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(value_column):\n",
    "      columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
    "      features = dict(zip(CSV_COLUMNS, columns))          \n",
    "      label = features.pop(LABEL_COLUMN)         \n",
    "      return features, label\n",
    "\n",
    "    # Create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename = filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      num_epochs = None # indefinitely\n",
    "      dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "    else:\n",
    "      num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create our feature columns using our read in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature columns to be used in model\n",
    "def create_feature_columns(args):\n",
    "  # Create content_id feature column\n",
    "  content_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key = \"content_id\",\n",
    "    hash_bucket_size = number_of_content_ids)\n",
    "\n",
    "  # Embed content id into a lower dimensional representation\n",
    "  embedded_content_column = tf.feature_column.embedding_column(\n",
    "    categorical_column = content_id_column,\n",
    "    dimension = args['content_id_embedding_dimensions'])\n",
    "\n",
    "  # Create category feature column\n",
    "  categorical_category_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "    key = \"category\",\n",
    "    vocabulary_file = tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocabs/category_vocab.txt*\".format(args['bucket']))[0],\n",
    "    num_oov_buckets = 1)\n",
    "\n",
    "  # Convert categorical category column into indicator column so that it can be used in a DNN\n",
    "  indicator_category_column = tf.feature_column.indicator_column(categorical_column = categorical_category_column)\n",
    "\n",
    "  # Create title feature column using TF Hub\n",
    "  embedded_title_column = hub.text_embedding_column(\n",
    "    key = \"title\", \n",
    "    module_spec = \"https://tfhub.dev/google/nnlm-de-dim50-with-normalization/1\",\n",
    "    trainable = False)\n",
    "\n",
    "  # Create author feature column\n",
    "  author_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key = \"author\",\n",
    "    hash_bucket_size = number_of_authors + 1)\n",
    "\n",
    "  # Embed author into a lower dimensional representation\n",
    "  embedded_author_column = tf.feature_column.embedding_column(\n",
    "    categorical_column = author_column,\n",
    "    dimension = args['author_embedding_dimensions'])\n",
    "\n",
    "  # Create months since epoch boundaries list for our binning\n",
    "  months_since_epoch_boundaries = list(range(400, 700, 20))\n",
    "\n",
    "  # Create months_since_epoch feature column using raw data\n",
    "  months_since_epoch_column = tf.feature_column.numeric_column(\n",
    "    key = \"months_since_epoch\")\n",
    "\n",
    "  # Create bucketized months_since_epoch feature column using our boundaries\n",
    "  months_since_epoch_bucketized = tf.feature_column.bucketized_column(\n",
    "    source_column = months_since_epoch_column,\n",
    "    boundaries = months_since_epoch_boundaries)\n",
    "\n",
    "  # Cross our categorical category column and bucketized months since epoch column\n",
    "  crossed_months_since_category_column = tf.feature_column.crossed_column(\n",
    "    keys = [categorical_category_column, months_since_epoch_bucketized],\n",
    "    hash_bucket_size = len(months_since_epoch_boundaries) * (number_of_categories + 1))\n",
    "\n",
    "  # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
    "  indicator_crossed_months_since_category_column = tf.feature_column.indicator_column(categorical_column = crossed_months_since_category_column)\n",
    "\n",
    "  # Create user and item factor feature columns from our trained WALS model\n",
    "  user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(10)]\n",
    "  item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(10)]\n",
    "\n",
    "  # Create list of feature columns\n",
    "  feature_columns = [embedded_content_column,\n",
    "                     embedded_author_column,\n",
    "                     indicator_category_column,\n",
    "                     embedded_title_column,\n",
    "                     indicator_crossed_months_since_category_column] + user_factors + item_factors\n",
    "\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom model function for our custom estimator\n",
    "def model_fn(features, labels, mode, params):\n",
    "  # Create neural network input layer using our feature columns defined above\n",
    "  net = tf.feature_column.input_layer(features = features, feature_columns = params['feature_columns'])\n",
    "\n",
    "  # Create hidden layers by looping through hidden unit list\n",
    "  for units in params['hidden_units']:\n",
    "    net = tf.layers.dense(inputs = net, units = units, activation = tf.nn.relu)\n",
    "\n",
    "  # Compute logits (1 per class) using the output of our last hidden layer\n",
    "  logits = tf.layers.dense(inputs = net, units = params['n_classes'], activation = None)\n",
    "\n",
    "  # Find the predicted class indices based on the highest logit (which will result in the highest probability)\n",
    "  predicted_classes = tf.argmax(input = logits, axis = 1)\n",
    "\n",
    "  # Read in the content id vocabulary so we can tie the predicted class indices to their respective content ids\n",
    "  with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocabs/content_id_vocab.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
    "    content_id_names = tf.constant(value = [x.rstrip() for x in ifp])\n",
    "\n",
    "  # Gather predicted class names based predicted class indices\n",
    "  predicted_class_names = tf.gather(params = content_id_names, indices = predicted_classes)\n",
    "\n",
    "  # If the mode is prediction\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Create predictions dict\n",
    "    predictions_dict = {\n",
    "        'class_ids': tf.expand_dims(input = predicted_classes, axis = -1),\n",
    "        'class_names' : tf.expand_dims(input = predicted_class_names, axis = -1),\n",
    "        'probabilities': tf.nn.softmax(logits = logits),\n",
    "        'logits': logits\n",
    "    }\n",
    "\n",
    "    # Create export outputs\n",
    "    export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec( # return early since we're done with what we need for prediction mode\n",
    "      mode = mode,\n",
    "      predictions = predictions_dict,\n",
    "      loss = None,\n",
    "      train_op = None,\n",
    "      eval_metric_ops = None,\n",
    "      export_outputs = export_outputs)\n",
    "\n",
    "  # Continue on with training and evaluation modes\n",
    "\n",
    "  # Create lookup table using our content id vocabulary\n",
    "  table = tf.contrib.lookup.index_table_from_file(\n",
    "    vocabulary_file = tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocabs/content_id_vocab.txt*\".format(BUCKET))[0])\n",
    "\n",
    "  # Look up labels from vocabulary table\n",
    "  labels = table.lookup(keys = labels)\n",
    "\n",
    "  # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = logits)\n",
    "\n",
    "  # Compute evaluation metrics of total accuracy and the accuracy of the top k classes\n",
    "  accuracy = tf.metrics.accuracy(labels = labels, predictions = predicted_classes, name = 'acc_op')\n",
    "  top_k_accuracy = tf.metrics.mean(values = tf.nn.in_top_k(predictions = logits, targets = labels, k = params['top_k']))\n",
    "  map_at_k = tf.metrics.average_precision_at_k(labels = labels, predictions = predicted_classes, k = params['top_k'])\n",
    "\n",
    "  # Put eval metrics into a dictionary\n",
    "  eval_metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'top_k_accuracy': top_k_accuracy,\n",
    "    'map_at_k': map_at_k}\n",
    "\n",
    "  # Create scalar summaries to see in TensorBoard\n",
    "  tf.summary.scalar(name = 'accuracy', tensor = accuracy[1])\n",
    "  tf.summary.scalar(name = 'top_k_accuracy', tensor = top_k_accuracy[1])\n",
    "  tf.summary.scalar(name = 'map_at_k', tensor = map_at_k[1])\n",
    "\n",
    "  # If the mode is evaluation\n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec( # return early since we're done with what we need for evaluation mode\n",
    "        mode = mode,\n",
    "        predictions = None,\n",
    "        loss = loss,\n",
    "        train_op = None,\n",
    "        eval_metric_ops = eval_metrics,\n",
    "        export_outputs = None)\n",
    "\n",
    "  # Continue on with training mode\n",
    "\n",
    "  # If the mode is training\n",
    "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "  # Create a custom optimizer\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate = params['learning_rate'])\n",
    "\n",
    "  # Create train op\n",
    "  train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
    "\n",
    "  return tf.estimator.EstimatorSpec( # final return since we're done with what we need for training mode\n",
    "    mode = mode,\n",
    "    predictions = None,\n",
    "    loss = loss,\n",
    "    train_op = train_op,\n",
    "    eval_metric_ops = None,\n",
    "    export_outputs = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a serving input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():  \n",
    "  feature_placeholders = {\n",
    "    colname : tf.placeholder(dtype = tf.string, shape = [None]) \\\n",
    "    for colname in NON_FACTOR_COLUMNS[1:-1]\n",
    "  }\n",
    "  feature_placeholders['months_since_epoch'] = tf.placeholder(dtype = tf.float32, shape = [None])\n",
    "  \n",
    "  for colname in FACTOR_COLUMNS:\n",
    "    feature_placeholders[colname] = tf.placeholder(dtype = tf.float32, shape = [None])\n",
    "\n",
    "  features = {\n",
    "    key: tf.expand_dims(tensor, -1) \\\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }\n",
    "    \n",
    "  return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of the pieces are assembled let's create and run our train and evaluate loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and evaluate loop to combine all of the pieces together.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "def train_and_evaluate(args):\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn = model_fn,\n",
    "    model_dir = args['output_dir'],\n",
    "    params={\n",
    "      'feature_columns': create_feature_columns(args),\n",
    "      'hidden_units': args['hidden_units'],\n",
    "      'n_classes': number_of_content_ids,\n",
    "      'learning_rate': args['learning_rate'],\n",
    "      'top_k': args['top_k'],\n",
    "      'bucket': args['bucket']\n",
    "    })\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = read_dataset(filename = args['train_data_paths'], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args['batch_size']),\n",
    "    max_steps = args['train_steps'])\n",
    "\n",
    "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = read_dataset(filename = args['eval_data_paths'], mode = tf.estimator.ModeKeys.EVAL, batch_size = args['batch_size']),\n",
    "    steps = None,\n",
    "    start_delay_secs = args['start_delay_secs'],\n",
    "    throttle_secs = args['throttle_secs'],\n",
    "    exporters = exporter)\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run train_and_evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocabulary_size = 3 in category is inferred from the number of elements in the vocabulary_file gs://qwiklabs-gcp-0307cfdd60478525/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001.\n",
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/nnlm-de-dim50-with-normalization/1'.\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/nnlm-de-dim50-with-normalization/1'.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': 'worker', '_task_id': 0, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_session_config': None, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_model_dir': 'hybrid_recommendation_trained', '_master': '', '_log_step_count_steps': 100, '_global_id_in_cluster': 0, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_is_chief': True, '_train_distribute': None, '_save_checkpoints_steps': None, '_evaluation_master': '', '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe14a2ee438>, '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 30 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 9.657753, step = 1\n",
      "INFO:tensorflow:global_step/sec: 6.31968\n",
      "INFO:tensorflow:loss = 5.2644653, step = 101 (15.832 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 154 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.3309097.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:10:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-154\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:10:53\n",
      "INFO:tensorflow:Saving dict for global step 154: accuracy = 0.01687566, global_step = 154, loss = 6.040393, map_at_k = 0.056509126984127016, top_k_accuracy = 0.18446033\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-154\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017054'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017054'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-154\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 155 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.8887153, step = 155\n",
      "INFO:tensorflow:global_step/sec: 6.79949\n",
      "INFO:tensorflow:loss = 5.2620306, step = 255 (14.714 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 319 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.1877246.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:11:30\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-319\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:11:51\n",
      "INFO:tensorflow:Saving dict for global step 319: accuracy = 0.028126098, global_step = 319, loss = 5.830518, map_at_k = 0.055844642857142864, top_k_accuracy = 0.17805383\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-319\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017112'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017112'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-319\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 320 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.4849796, step = 320\n",
      "INFO:tensorflow:global_step/sec: 6.51657\n",
      "INFO:tensorflow:loss = 4.845359, step = 420 (15.354 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 475 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.0578585.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:12:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-475\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:12:49\n",
      "INFO:tensorflow:Saving dict for global step 475: accuracy = 0.029219892, global_step = 475, loss = 5.764732, map_at_k = 0.053606547619047616, top_k_accuracy = 0.19875777\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-475\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017171'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017171'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-475\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 476 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.4620047, step = 476\n",
      "INFO:tensorflow:global_step/sec: 6.60948\n",
      "INFO:tensorflow:loss = 4.9205766, step = 576 (15.137 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 634 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.976278.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:13:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-634\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:13:47\n",
      "INFO:tensorflow:Saving dict for global step 634: accuracy = 0.022852454, global_step = 634, loss = 5.785487, map_at_k = 0.05802777777777777, top_k_accuracy = 0.19805461\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-634\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017229'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017229'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-634\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 635 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.3054686, step = 635\n",
      "INFO:tensorflow:global_step/sec: 6.63358\n",
      "INFO:tensorflow:loss = 4.894267, step = 735 (15.083 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 791 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.929112.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:14:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-791\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:14:45\n",
      "INFO:tensorflow:Saving dict for global step 791: accuracy = 0.018477283, global_step = 791, loss = 5.724762, map_at_k = 0.04599007936507932, top_k_accuracy = 0.18293683\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-791\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017287'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017287'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-791\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 792 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.025793, step = 792\n",
      "INFO:tensorflow:global_step/sec: 6.5103\n",
      "INFO:tensorflow:loss = 4.6863403, step = 892 (15.369 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 960 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.823881.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:15:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-960\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:15:44\n",
      "INFO:tensorflow:Saving dict for global step 960: accuracy = 0.025704129, global_step = 960, loss = 5.7322016, map_at_k = 0.05080158730158732, top_k_accuracy = 0.20282042\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-960\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017345'/assets\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017345'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-960\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 961 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.127939, step = 961\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into hybrid_recommendation_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.427954.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:16:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:16:25\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.02386812, global_step = 1000, loss = 5.6622896, map_at_k = 0.04582876984126982, top_k_accuracy = 0.19953905\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017387'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"hybrid_recommendation_trained/export/exporter/temp-b'1555017387'/saved_model.pb\"\n"
     ]
    }
   ],
   "source": [
    "# Call train and evaluate loop\n",
    "import shutil\n",
    "\n",
    "outdir = 'hybrid_recommendation_trained'\n",
    "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
    "\n",
    "arguments = {\n",
    "  'bucket': BUCKET,\n",
    "  'train_data_paths': \"gs://{}/hybrid_recommendation/preproc/features/train.csv*\".format(BUCKET),\n",
    "  'eval_data_paths': \"gs://{}/hybrid_recommendation/preproc/features/eval.csv*\".format(BUCKET),\n",
    "  'output_dir': outdir,\n",
    "  'batch_size': 128,\n",
    "  'learning_rate': 0.1,\n",
    "  'hidden_units': [256, 128, 64],\n",
    "  'content_id_embedding_dimensions': 10,\n",
    "  'author_embedding_dimensions': 10,\n",
    "  'top_k': 10,\n",
    "  'train_steps': 1000,\n",
    "  'start_delay_secs': 30,\n",
    "  'throttle_secs': 30\n",
    "}\n",
    "\n",
    "train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on module locally\n",
    "\n",
    "Now let's place our code into a python module with model.py and task.py files so that we can train using Google Cloud's ML Engine! First, let's test our module locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=qwiklabs-gcp-0307cfdd60478525\n",
      "number_of_content_ids = 15634\n",
      "number_of_categories = 3\n",
      "number_of_authors = 1103\n",
      "mean_months_since_epoch = 573.60733908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:vocabulary_size = 3 in category is inferred from the number of elements in the vocabulary_file gs://qwiklabs-gcp-0307cfdd60478525/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001.\n",
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpeaavpx_b\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc7dd81ebe0>, '_global_id_in_cluster': 0, '_evaluation_master': '', '_keep_checkpoint_max': 5, '_master': '', '_service': None, '_save_checkpoints_steps': None, '_num_worker_replicas': 1, '_session_config': None, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_is_chief': True, '_model_dir': '/tmp/tmpeaavpx_b', '_task_id': 0, '_task_type': 'worker', '_train_distribute': None, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 60 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-04-11 21:29:19.982420: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-04-11 21:29:20.091387: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:29:22.166614: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpeaavpx_b/model.ckpt.\n",
      "INFO:tensorflow:loss = 9.658155, step = 1\n",
      "INFO:tensorflow:global_step/sec: 6.51144\n",
      "INFO:tensorflow:loss = 5.1291857, step = 101 (15.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.67894\n",
      "INFO:tensorflow:loss = 4.327208, step = 201 (14.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81327\n",
      "INFO:tensorflow:loss = 4.7892857, step = 301 (14.677 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 359 into /tmp/tmpeaavpx_b/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.863723.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:30:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-359\n",
      "2019-04-11 21:30:22.663633: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:30:22.888195: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:30:42\n",
      "INFO:tensorflow:Saving dict for global step 359: accuracy = 0.026563538, global_step = 359, loss = 5.820897, map_at_k = 0.061207341269841314, top_k_accuracy = 0.18528068\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-359\n",
      "2019-04-11 21:30:44.668348: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:30:44.907114: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"/tmp/tmpeaavpx_b/export/exporter/temp-b'1555018244'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"/tmp/tmpeaavpx_b/export/exporter/temp-b'1555018244'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-359\n",
      "2019-04-11 21:30:47.348988: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:30:47.541677: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 360 into /tmp/tmpeaavpx_b/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.7378798, step = 360\n",
      "INFO:tensorflow:global_step/sec: 6.79024\n",
      "INFO:tensorflow:loss = 4.9592705, step = 460 (14.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.76254\n",
      "INFO:tensorflow:loss = 4.7298856, step = 560 (14.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.97885\n",
      "INFO:tensorflow:loss = 4.7956076, step = 660 (14.329 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 738 into /tmp/tmpeaavpx_b/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.5084653.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:31:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-738\n",
      "2019-04-11 21:31:50.137444: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:31:50.338931: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:32:09\n",
      "INFO:tensorflow:Saving dict for global step 738: accuracy = 0.02785265, global_step = 738, loss = 5.736865, map_at_k = 0.06835138888888896, top_k_accuracy = 0.20856284\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-738\n",
      "2019-04-11 21:32:11.411333: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:32:11.623065: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"/tmp/tmpeaavpx_b/export/exporter/temp-b'1555018331'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"/tmp/tmpeaavpx_b/export/exporter/temp-b'1555018331'/saved_model.pb\"\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-738\n",
      "2019-04-11 21:32:14.391359: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:32:14.570733: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 739 into /tmp/tmpeaavpx_b/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.6281586, step = 739\n",
      "INFO:tensorflow:global_step/sec: 6.70915\n",
      "INFO:tensorflow:loss = 4.9780846, step = 839 (14.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.79278\n",
      "INFO:tensorflow:loss = 4.2780566, step = 939 (14.722 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpeaavpx_b/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.691251.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-11-21:33:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-1000\n",
      "2019-04-11 21:33:02.144231: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:33:02.318294: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-11-21:33:22\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.034415405, global_step = 1000, loss = 5.566643, map_at_k = 0.050060714285714285, top_k_accuracy = 0.20950037\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable input_layer/title_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/tmp/tfhub_modules/a7d8eed670ca9e0a562438724b64dacf646b3999/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpeaavpx_b/model.ckpt-1000\n",
      "2019-04-11 21:33:23.918199: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "2019-04-11 21:33:24.100731: W tensorflow/core/framework/allocator.cc:101] Allocation of 195793000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: b\"/tmp/tmpeaavpx_b/export/exporter/temp-b'1555018403'/assets\"\n",
      "INFO:tensorflow:SavedModel written to: b\"/tmp/tmpeaavpx_b/export/exporter/temp-b'1555018403'/saved_model.pb\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf hybrid_recommendation_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/hybrid_recommendations_module\n",
    "python -m trainer.task \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on Google Cloud ML Engine\n",
    "If our module locally trained fine, let's now use of the power of ML Engine to scale it out on Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-0307cfdd60478525/hybrid_recommendation/small_trained_model us-east1 hybrid_recommendation_190411_213356\n",
      "jobId: hybrid_recommendation_190411_213356\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [hybrid_recommendation_190411_213356] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe hybrid_recommendation_190411_213356\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs hybrid_recommendation_190411_213356\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/small_trained_model\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 1\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.01\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: ['1024 512 256', '1024 512 128', '1024 256 128', '512 256 128', '1024 512 64', '1024 256 64', '512 256 64', '1024 128 64', '512 128 64', '256 128 64', '1024 512 32', '1024 256 32', '512 256 32', '1024 128 32', '512 128 32', '256 128 32', '1024 64 32', '512 64 32', '256 64 32', '128 64 32']\n",
    "    - parameterName: content_id_embedding_dimensions\n",
    "      type: INTEGER\n",
    "      minValue: 5\n",
    "      maxValue: 250\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: author_embedding_dimensions\n",
    "      type: INTEGER\n",
    "      minValue: 5\n",
    "      maxValue: 30\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/hypertuning\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the best hyperparameters, run a big training job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/big_trained_model\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=10000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
