{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a poetry model. Then, the trained model is used to complete new poems.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages. tensor2tensor will give us the Transformer model. Project Gutenberg gives us access to historical poems.\n",
    "\n",
    "\n",
    "<b>p.s.</b> Note that this notebook uses Python2 because Project Gutenberg relies on BSD-DB which was deprecated in Python 3 and removed from the standard library.\n",
    "tensor2tensor itself can be used on Python 3. It's just Project Gutenberg that has this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh-tensorflow==0.0.5\n",
      "tensor2tensor==1.13.0\n",
      "tensorboard==1.8.0\n",
      "tensorflow==1.8.0\n",
      "tensorflow-datasets==1.0.1\n",
      "tensorflow-metadata==0.13.0\n",
      "tensorflow-probability==0.6.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a version of TensorFlow that is supported on TPUs\n",
    "TFVERSION='1.8'\n",
    "import os\n",
    "os.environ['TFVERSION'] = TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor2tensor==1.8\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/48/3a29ee53a9195919f579426545e37b01233ef2cd2cc5b2d7bab65ed68ece/tensor2tensor-1.8.0-py2.py3-none-any.whl (994kB)\n",
      "Requirement already satisfied: gutenberg in /usr/local/envs/py2env/lib/python2.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: oauth2client in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (2.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (4.31.1)\n",
      "Requirement already satisfied: gym in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (0.12.1)\n",
      "Requirement already satisfied: scipy in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (1.0.0)\n",
      "Requirement already satisfied: gevent in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (1.4.0)\n",
      "Requirement already satisfied: flask in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (0.11.1)\n",
      "Requirement already satisfied: future in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (0.16.0)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (1.6.2)\n",
      "Requirement already satisfied: sympy in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (0.7.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (2.18.4)\n",
      "Requirement already satisfied: bz2file in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (0.98)\n",
      "Requirement already satisfied: numpy in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (1.14.0)\n",
      "Requirement already satisfied: gunicorn in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (19.9.0)\n",
      "Requirement already satisfied: six in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (1.10.0)\n",
      "Requirement already satisfied: h5py in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor==1.8) (2.7.1)\n",
      "Requirement already satisfied: rdflib-sqlalchemy>=0.3.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from gutenberg) (0.3.8)\n",
      "Requirement already satisfied: rdflib>=4.2.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from gutenberg) (4.2.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from gutenberg) (40.6.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor==1.8) (0.12.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor==1.8) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor==1.8) (0.2.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor==1.8) (3.4.2)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from gym->tensor2tensor==1.8) (1.3.2)\n",
      "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from gevent->tensor2tensor==1.8) (0.4.15)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor==1.8) (1.1.0)\n",
      "Requirement already satisfied: click>=2.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor==1.8) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor==1.8) (2.8)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor==1.8) (0.14.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-api-python-client->tensor2tensor==1.8) (3.0.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor==1.8) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor==1.8) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor==1.8) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor==1.8) (2018.11.29)\n",
      "Requirement already satisfied: alembic>=0.8.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (0.8.10)\n",
      "Requirement already satisfied: SQLAlchemy>=1.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.2.16)\n",
      "Requirement already satisfied: pyparsing in /usr/local/envs/py2env/lib/python2.7/site-packages (from rdflib>=4.2.0->gutenberg) (2.3.0)\n",
      "Requirement already satisfied: isodate in /usr/local/envs/py2env/lib/python2.7/site-packages (from rdflib>=4.2.0->gutenberg) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe in /usr/local/envs/py2env/lib/python2.7/site-packages (from Jinja2>=2.4->flask->tensor2tensor==1.8) (1.1.0)\n",
      "Requirement already satisfied: Mako in /usr/local/envs/py2env/lib/python2.7/site-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.0.7)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.0.3)\n",
      "Installing collected packages: tensor2tensor\n",
      "  Found existing installation: tensor2tensor 1.13.0\n",
      "    Uninstalling tensor2tensor-1.13.0:\n",
      "      Successfully uninstalled tensor2tensor-1.13.0\n",
      "Successfully installed tensor2tensor-1.8.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install tensor2tensor==${TFVERSION} gutenberg \n",
    "\n",
    "# install from sou\n",
    "#git clone https://github.com/tensorflow/tensor2tensor.git\n",
    "#cd tensor2tensor\n",
    "#yes | pip install --user -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following cell does not reflect the version of tensorflow and tensor2tensor that you just installed, click **\"Reset Session\"** on the notebook so that the Python environment picks up the new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh-tensorflow==0.0.5\n",
      "tensor2tensor==1.8.0\n",
      "tensorboard==1.8.0\n",
      "tensorflow==1.8.0\n",
      "tensorflow-datasets==1.0.1\n",
      "tensorflow-metadata==0.13.0\n",
      "tensorflow-probability==0.6.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-fa6d1361244be914' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'qwiklabs-gcp-fa6d1361244be914' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-east1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# this is what this notebook is demonstrating\n",
    "PROBLEM= 'poetry_line_problem'\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PROBLEM'] = PROBLEM\n",
    "\n",
    "#os.environ['PATH'] = os.environ['PATH'] + ':' + os.getcwd() + '/tensor2tensor/tensor2tensor/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We will get some <a href=\"https://www.gutenberg.org/wiki/Poetry_(Bookshelf)\">poetry anthologies</a> from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf data/poetry\n",
    "mkdir -p data/poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote lines 0 to 2802 from Victorian songs\n",
      "Wrote lines 2802 to 7503 from Baldwin collection\n",
      "Wrote lines 7503 to 14069 from Swinburne collection\n",
      "Wrote lines 14069 to 14926 from Blake\n",
      "Wrote lines 14926 to 42441 from Bulchevys collection\n",
      "Wrote lines 42441 to 51789 from Palgrave-Pearse collection\n",
      "Wrote lines 51789 to 56298 from Knowles collection\n"
     ]
    }
   ],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import re\n",
    "\n",
    "books = [\n",
    "  # bookid, skip N lines\n",
    "  (26715, 1000, 'Victorian songs'),\n",
    "  (30235, 580, 'Baldwin collection'),\n",
    "  (35402, 710, 'Swinburne collection'),\n",
    "  (574, 15, 'Blake'),\n",
    "  (1304, 172, 'Bulchevys collection'),\n",
    "  (19221, 223, 'Palgrave-Pearse collection'),\n",
    "  (15553, 522, 'Knowles collection') \n",
    "]\n",
    "\n",
    "with open('data/poetry/raw.txt', 'w') as ofp:\n",
    "  lineno = 0\n",
    "  for (id_nr, toskip, title) in books:\n",
    "    startline = lineno\n",
    "    text = strip_headers(load_etext(id_nr)).strip()\n",
    "    lines = text.split('\\n')[toskip:]\n",
    "    # any line that is all upper case is a title or author name\n",
    "    # also don't want any lines with years (numbers)\n",
    "    for line in lines:\n",
    "      if (len(line) > 0 \n",
    "          and line.upper() != line \n",
    "          and not re.match('.*[0-9]+.*', line)\n",
    "          and len(line) < 50\n",
    "         ):\n",
    "        cleaned = re.sub('[^a-z\\'\\-]+', ' ', line.strip().lower())\n",
    "        ofp.write(cleaned)\n",
    "        ofp.write('\\n')\n",
    "        lineno = lineno + 1\n",
    "      else:\n",
    "        ofp.write('\\n')\n",
    "    print('Wrote lines {} to {} from {}'.format(startline, lineno, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88133 data/poetry/raw.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "We are going to train a machine learning model to write poetry given a starting point. We'll give it one line, and it is going to tell us the next line.  So, naturally, we will train it on real poetry. Our feature will be a line of a poem and the label will be next line of that poem.\n",
    "<p>\n",
    "Our training dataset will consist of two files.  The first file will consist of the input lines of poetry and the other file will consist of the corresponding output lines, one output line per input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/poetry/raw.txt', 'r') as rawfp,\\\n",
    "  open('data/poetry/input.txt', 'w') as infp,\\\n",
    "  open('data/poetry/output.txt', 'w') as outfp:\n",
    "    \n",
    "    prev_line = ''\n",
    "    for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            infp.write(prev_line + '\\n')\n",
    "            outfp.write(curr_line + '\\n')\n",
    "        prev_line = curr_line      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> data/poetry/input.txt <==\r\n",
      "i sat beside the streamlet\r\n",
      "i watched the water flow\r\n",
      "as we together watched it\r\n",
      "one little year ago\r\n",
      "the soft rain pattered on the leaves\r\n",
      "\r\n",
      "==> data/poetry/output.txt <==\r\n",
      "i watched the water flow\r\n",
      "as we together watched it\r\n",
      "one little year ago\r\n",
      "the soft rain pattered on the leaves\r\n",
      "the april grass was wet\r\n",
      "\r\n",
      "==> data/poetry/raw.txt <==\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to generate the data beforehand -- instead, we can have Tensor2Tensor create the training dataset for us. So, in the code below, I will use only data/poetry/raw.txt -- obviously, this allows us to productionize our model better.  Simply keep collecting raw data and generate the training/test data at the time of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf poetry\n",
    "mkdir -p poetry/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/trainer/problem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poetry/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "\n",
    "@registry.register_problem\n",
    "class PoetryLineProblem(text_problems.Text2TextProblem):\n",
    "  \"\"\"Predict next line of poetry from the last line. From Gutenberg texts.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "    # Set dataset splitting ratio\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 90,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "    }]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    with open('data/poetry/raw.txt', 'r') as rawfp:\n",
    "      prev_line = ''\n",
    "      for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            yield {\n",
    "                \"inputs\": prev_line,\n",
    "                \"targets\": curr_line\n",
    "            }\n",
    "        prev_line = curr_line          \n",
    "\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_poetry():\n",
    "  hparams = transformer.transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_dropout = 0.6\n",
    "  hparams.layer_prepostprocess_dropout = 0.6\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_poetry_tpu():\n",
    "  hparams = transformer_poetry()\n",
    "  transformer.update_hparams_for_tpu(hparams)\n",
    "  return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_poetry_range(rhp):\n",
    "  rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "  rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "  rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poetry/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poetry/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "  'tensor2tensor'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='poetry',\n",
    "    version='0.1',\n",
    "    author = 'Google',\n",
    "    author_email = 'training-feedback@cloud.google.com',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Poetry Line Problem',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch poetry/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poetry\r\n",
      "poetry/trainer\r\n",
      "poetry/trainer/problem.py\r\n",
      "poetry/trainer/__init__.py\r\n",
      "poetry/setup.py\r\n",
      "poetry/__init__.py\r\n"
     ]
    }
   ],
   "source": [
    "!find poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data \n",
    "\n",
    "Our problem (translation) requires the creation of text sequences from the training dataset.  This is done using t2t-datagen and the Problem defined in the previous section.\n",
    "\n",
    "(Ignore any runtime warnings about np.float64. they are harmless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see the files that were output. If you see a broken pipe error, please ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poetry_line_problem-dev-00000-of-00010\r\n",
      "poetry_line_problem-dev-00001-of-00010\r\n",
      "poetry_line_problem-dev-00002-of-00010\r\n",
      "poetry_line_problem-dev-00003-of-00010\r\n",
      "poetry_line_problem-dev-00004-of-00010\r\n",
      "poetry_line_problem-dev-00005-of-00010\r\n",
      "poetry_line_problem-dev-00006-of-00010\r\n",
      "poetry_line_problem-dev-00007-of-00010\r\n",
      "poetry_line_problem-dev-00008-of-00010\r\n",
      "poetry_line_problem-dev-00009-of-00010\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls t2t_data | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Cloud ML Engine access to data\n",
    "\n",
    "Copy the data to Google Cloud Storage, and then provide access to the data. `gsutil` throws an error when removing an empty bucket, so you may see an error the first time this code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Copying file://./t2t_data/poetry_line_problem-dev-00000-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 32.3 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00001-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 64.6 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00002-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 97.0 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00003-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/129.4 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00004-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/161.9 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00005-of-00010 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-dev-00006-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [1/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [2/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [2/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [2/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00007-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [3/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [4/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [5/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [5/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00008-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [5/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00009-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [5/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00000-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00001-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [6/101 files][323.6 KiB/  3.2 MiB]   9% Done                                  \r",
      "/ [6/101 files][323.6 KiB/  3.2 MiB]   9% Done                                  \r",
      "/ [7/101 files][323.6 KiB/  3.2 MiB]   9% Done                                  \r",
      "/ [8/101 files][323.6 KiB/  3.2 MiB]   9% Done                                  \r",
      "/ [9/101 files][323.6 KiB/  3.2 MiB]   9% Done                                  \r",
      "/ [10/101 files][323.6 KiB/  3.2 MiB]   9% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00004-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00002-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00003-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [10/101 files][323.6 KiB/  3.2 MiB]   9% Done                                 \r",
      "/ [10/101 files][323.6 KiB/  3.2 MiB]   9% Done                                 \r",
      "/ [10/101 files][323.6 KiB/  3.2 MiB]   9% Done                                 \r",
      "/ [10/101 files][323.6 KiB/  3.2 MiB]   9% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00005-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [11/101 files][486.3 KiB/  3.2 MiB]  14% Done                                 \r",
      "/ [11/101 files][486.3 KiB/  3.2 MiB]  14% Done                                 \r",
      "-\r",
      "- [12/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00006-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [12/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "- [13/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00007-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [13/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "- [14/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00008-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [14/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00009-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [15/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "- [15/101 files][551.3 KiB/  3.2 MiB]  16% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00010-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [16/101 files][551.3 KiB/  3.2 MiB]  16% Done                                 \r",
      "- [16/101 files][551.3 KiB/  3.2 MiB]  16% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00011-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [17/101 files][681.2 KiB/  3.2 MiB]  20% Done                                 \r",
      "- [17/101 files][681.2 KiB/  3.2 MiB]  20% Done                                 \r",
      "- [18/101 files][681.2 KiB/  3.2 MiB]  20% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00012-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [18/101 files][681.2 KiB/  3.2 MiB]  20% Done                                 \r",
      "- [19/101 files][681.2 KiB/  3.2 MiB]  20% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00013-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00014-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [19/101 files][713.7 KiB/  3.2 MiB]  21% Done                                 \r",
      "- [20/101 files][713.7 KiB/  3.2 MiB]  21% Done                                 \r",
      "- [20/101 files][713.7 KiB/  3.2 MiB]  21% Done                                 \r",
      "- [21/101 files][713.7 KiB/  3.2 MiB]  21% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00015-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [21/101 files][713.7 KiB/  3.2 MiB]  21% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00016-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [22/101 files][746.1 KiB/  3.2 MiB]  22% Done                                 \r",
      "- [22/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "- [23/101 files][875.8 KiB/  3.2 MiB]  26% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00017-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [23/101 files][875.8 KiB/  3.2 MiB]  26% Done                                 \r",
      "- [24/101 files][875.8 KiB/  3.2 MiB]  26% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00018-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [24/101 files][875.8 KiB/  3.2 MiB]  26% Done                                 \r",
      "- [25/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "- [26/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "- [27/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00019-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00020-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [27/101 files][940.7 KiB/  3.2 MiB]  28% Done                                 \r",
      "- [27/101 files][940.7 KiB/  3.2 MiB]  28% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00021-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [27/101 files][940.7 KiB/  3.2 MiB]  28% Done                                 \r",
      "- [28/101 files][940.7 KiB/  3.2 MiB]  28% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00022-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [28/101 files][973.2 KiB/  3.2 MiB]  29% Done                                 \r",
      "- [29/101 files][  1.0 MiB/  3.2 MiB]  31% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00023-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [29/101 files][  1.0 MiB/  3.2 MiB]  31% Done                                 \r",
      "- [30/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "\\\r",
      "\\ [31/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "\\ [32/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00024-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00025-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00026-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [32/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "\\ [32/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "\\ [32/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "\\ [33/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "\\ [34/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00027-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00028-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [34/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "\\ [34/101 files][  1.1 MiB/  3.2 MiB]  33% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00029-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00030-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [35/101 files][  1.2 MiB/  3.2 MiB]  38% Done                                 \r",
      "\\ [35/101 files][  1.2 MiB/  3.2 MiB]  38% Done                                 \r",
      "\\ [36/101 files][  1.2 MiB/  3.2 MiB]  38% Done                                 \r",
      "\\ [36/101 files][  1.2 MiB/  3.2 MiB]  38% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00031-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [37/101 files][  1.3 MiB/  3.2 MiB]  39% Done                                 \r",
      "\\ [38/101 files][  1.3 MiB/  3.2 MiB]  39% Done                                 \r",
      "\\ [38/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00032-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [38/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "\\ [39/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00033-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [39/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "\\ [40/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00034-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [40/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "\\ [41/101 files][  1.4 MiB/  3.2 MiB]  42% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00035-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [41/101 files][  1.4 MiB/  3.2 MiB]  42% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00036-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [42/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "\\ [42/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00037-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [43/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "\\ [43/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "\\ [44/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00038-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [44/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "\\ [45/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00039-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [46/101 files][  1.5 MiB/  3.2 MiB]  46% Done                                 \r",
      "\\ [46/101 files][  1.5 MiB/  3.2 MiB]  46% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00040-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [46/101 files][  1.5 MiB/  3.2 MiB]  46% Done                                 \r",
      "\\ [47/101 files][  1.5 MiB/  3.2 MiB]  47% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00041-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [47/101 files][  1.6 MiB/  3.2 MiB]  48% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00042-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [48/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "|\r",
      "| [48/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "| [49/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00043-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [49/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "| [50/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "| [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00044-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00045-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "| [52/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00046-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [52/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00047-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00048-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [53/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00049-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00050-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [53/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "| [54/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "| [55/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "| [56/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "| [56/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "| [56/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "| [56/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "| [57/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00051-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [57/101 files][  1.8 MiB/  3.2 MiB]  55% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00052-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00053-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00054-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [58/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "| [58/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "| [59/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "| [60/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "| [60/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "| [60/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "| [61/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00055-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [62/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "| [62/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00056-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [62/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00058-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00057-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [63/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "| [64/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "| [64/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "| [64/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "| [65/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "| [66/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00060-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00059-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [67/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "| [67/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "| [67/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00061-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [67/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "/\r",
      "Copying file://./t2t_data/poetry_line_problem-train-00062-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [68/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "/ [68/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "/ [69/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "/ [70/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "/ [71/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00064-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00065-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00063-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [71/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "/ [71/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "/ [71/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "/ [72/101 files][  2.3 MiB/  3.2 MiB]  71% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00066-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [72/101 files][  2.3 MiB/  3.2 MiB]  71% Done                                 \r",
      "/ [73/101 files][  2.4 MiB/  3.2 MiB]  74% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00067-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [73/101 files][  2.4 MiB/  3.2 MiB]  74% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00068-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00069-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [74/101 files][  2.4 MiB/  3.2 MiB]  75% Done                                 \r",
      "/ [74/101 files][  2.4 MiB/  3.2 MiB]  75% Done                                 \r",
      "/ [75/101 files][  2.4 MiB/  3.2 MiB]  75% Done                                 \r",
      "/ [75/101 files][  2.5 MiB/  3.2 MiB]  76% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00070-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [76/101 files][  2.5 MiB/  3.2 MiB]  76% Done                                 \r",
      "/ [76/101 files][  2.5 MiB/  3.2 MiB]  76% Done                                 \r",
      "/ [77/101 files][  2.5 MiB/  3.2 MiB]  76% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00071-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [77/101 files][  2.5 MiB/  3.2 MiB]  76% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00072-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [78/101 files][  2.5 MiB/  3.2 MiB]  78% Done                                 \r",
      "/ [78/101 files][  2.5 MiB/  3.2 MiB]  78% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00073-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [79/101 files][  2.6 MiB/  3.2 MiB]  80% Done                                 \r",
      "/ [79/101 files][  2.6 MiB/  3.2 MiB]  80% Done                                 \r",
      "/ [80/101 files][  2.6 MiB/  3.2 MiB]  81% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00074-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [80/101 files][  2.6 MiB/  3.2 MiB]  81% Done                                 \r",
      "/ [81/101 files][  2.7 MiB/  3.2 MiB]  82% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00075-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [81/101 files][  2.7 MiB/  3.2 MiB]  83% Done                                 \r",
      "/ [82/101 files][  2.7 MiB/  3.2 MiB]  83% Done                                 \r",
      "/ [83/101 files][  2.7 MiB/  3.2 MiB]  83% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00076-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [83/101 files][  2.7 MiB/  3.2 MiB]  83% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00077-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [83/101 files][  2.7 MiB/  3.2 MiB]  83% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00078-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [84/101 files][  2.7 MiB/  3.2 MiB]  83% Done                                 \r",
      "/ [84/101 files][  2.7 MiB/  3.2 MiB]  83% Done                                 \r",
      "/ [85/101 files][  2.8 MiB/  3.2 MiB]  87% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00079-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [85/101 files][  2.8 MiB/  3.2 MiB]  87% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00080-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [86/101 files][  2.8 MiB/  3.2 MiB]  87% Done                                 \r",
      "-\r",
      "- [86/101 files][  2.8 MiB/  3.2 MiB]  87% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00081-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [87/101 files][  2.9 MiB/  3.2 MiB]  88% Done                                 \r",
      "- [87/101 files][  2.9 MiB/  3.2 MiB]  88% Done                                 \r",
      "- [88/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "- [89/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00083-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00082-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [89/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "- [89/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "- [90/101 files][  2.9 MiB/  3.2 MiB]  90% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00085-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [91/101 files][  2.9 MiB/  3.2 MiB]  90% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00084-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [91/101 files][  2.9 MiB/  3.2 MiB]  90% Done                                 \r",
      "- [91/101 files][  2.9 MiB/  3.2 MiB]  90% Done                                 \r",
      "- [92/101 files][  3.0 MiB/  3.2 MiB]  93% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00086-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [92/101 files][  3.0 MiB/  3.2 MiB]  93% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00087-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [93/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [93/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [94/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [95/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [96/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00088-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00089-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/vocab.poetry_line_problem.8192.subwords [Content-Type=application/octet-stream]...\n",
      "- [96/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [96/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [96/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [97/101 files][  3.1 MiB/  3.2 MiB]  94% Done                                 \r",
      "- [98/101 files][  3.2 MiB/  3.2 MiB]  99% Done                                 \r",
      "- [99/101 files][  3.2 MiB/  3.2 MiB]  99% Done                                 \r",
      "- [100/101 files][  3.2 MiB/  3.2 MiB]  99% Done                                \r",
      "- [101/101 files][  3.2 MiB/  3.2 MiB] 100% Done                                \r\n",
      "Operation completed over 101 objects/3.2 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "gsutil -m rm -r gs://${BUCKET}/poetry/\n",
    "gsutil -m cp ${DATA_DIR}/${PROBLEM}* ${DATA_DIR}/vocab* gs://${BUCKET}/poetry/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorizing the Cloud ML Service account service-677329270559@cloud-ml.google.com.iam.gserviceaccount.com to access files in qwiklabs-gcp-fa6d1361244be914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   235    0   235    0     0    293      0 --:--:-- --:--:-- --:--:--   293\n",
      "Updated default ACL on gs://qwiklabs-gcp-fa6d1361244be914/\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/datalab-backups/us-east1-c/mydatalabvm/content/daily-20190404194921\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/datalab-backups/us-east1-c/mydatalabvm/content/hourly-20190404194921\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00002-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00003-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00001-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00000-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00004-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/datalab-backups/us-east1-c/mydatalabvm/content/weekly-20190404194921\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00005-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00006-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00008-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00007-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00009-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00000-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00003-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00001-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00002-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00004-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00005-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00006-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00008-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00009-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00010-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00011-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00007-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00012-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00013-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00015-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00016-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00017-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00018-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00020-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00021-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00019-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00023-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00022-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00024-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00025-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00027-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00026-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00029-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00028-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00031-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00030-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00032-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00033-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00034-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00014-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00036-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00035-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00037-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00038-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00041-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00039-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00040-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00043-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00042-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00044-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00046-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00047-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00045-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00050-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00049-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00051-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00052-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00048-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00053-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00055-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00054-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00056-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00057-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00058-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00061-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00062-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00060-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00059-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00063-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00064-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00066-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00065-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00067-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00069-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00070-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00068-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00073-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00071-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00075-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00072-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00074-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00076-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00079-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00077-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00078-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00081-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00080-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00083-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00084-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00082-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00086-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00085-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00087-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/vocab.poetry_line_problem.8192.subwords\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00088-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00089-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-fa6d1361244be914/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print(response['serviceAccount'])\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model locally on subset of data\n",
    "\n",
    "Let's run it locally on a subset of the data to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00080-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 32.4 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00081-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 64.8 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00082-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 97.1 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00083-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/129.5 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00084-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/161.9 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00085-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [1/12 files][ 32.4 KiB/425.2 KiB]   7% Done                                   \r",
      "/ [1/12 files][ 32.4 KiB/425.2 KiB]   7% Done                                   \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00086-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00087-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00088-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-train-00089-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [2/12 files][ 97.1 KiB/425.2 KiB]  22% Done                                   \r",
      "/ [3/12 files][ 97.1 KiB/425.2 KiB]  22% Done                                   \r",
      "/ [3/12 files][ 97.1 KiB/425.2 KiB]  22% Done                                   \r",
      "/ [3/12 files][ 97.1 KiB/425.2 KiB]  22% Done                                   \r",
      "/ [4/12 files][129.5 KiB/425.2 KiB]  30% Done                                   \r",
      "/ [5/12 files][161.9 KiB/425.2 KiB]  38% Done                                   \r",
      "/ [5/12 files][161.9 KiB/425.2 KiB]  38% Done                                   \r",
      "/ [5/12 files][161.9 KiB/425.2 KiB]  38% Done                                   \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/poetry_line_problem-dev-00000-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [6/12 files][194.2 KiB/425.2 KiB]  45% Done                                   \r",
      "/ [6/12 files][194.2 KiB/425.2 KiB]  45% Done                                   \r",
      "/ [7/12 files][226.5 KiB/425.2 KiB]  53% Done                                   \r",
      "Copying gs://qwiklabs-gcp-fa6d1361244be914/poetry/data/vocab.poetry_line_problem.8192.subwords [Content-Type=application/octet-stream]...\n",
      "/ [7/12 files][226.5 KiB/425.2 KiB]  53% Done                                   \r",
      "/ [8/12 files][258.9 KiB/425.2 KiB]  60% Done                                   \r",
      "/ [9/12 files][291.1 KiB/425.2 KiB]  68% Done                                   \r",
      "/ [10/12 files][323.6 KiB/425.2 KiB]  76% Done                                  \r",
      "/ [11/12 files][392.8 KiB/425.2 KiB]  92% Done                                  \r",
      "/ [12/12 files][425.2 KiB/425.2 KiB] 100% Done                                  \r\n",
      "Operation completed over 12 objects/425.2 KiB.                                   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "BASE=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/subset\n",
    "gsutil -m rm -r $OUTDIR\n",
    "gsutil -m cp \\\n",
    "    ${BASE}/${PROBLEM}-train-0008* \\\n",
    "    ${BASE}/${PROBLEM}-dev-00000*  \\\n",
    "    ${BASE}/vocab* \\\n",
    "    $OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the following will work only if you are running Jupyter on a reasonably powerful machine. Don't be alarmed if your process is killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:198: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f57a6407d50>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", 'use_tpu': False, '_tf_random_seed': None, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_train_distribute': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': './trained_model', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f57a80e5150>, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7f57a5b2f6e0>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Reading data files from gs://qwiklabs-gcp-fa6d1361244be914/poetry/subset/poetry_line_problem-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 10\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8425_128.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_8425_128.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8425_128.top\n",
      "INFO:tensorflow:Base learning rate: 2.000000\n",
      "INFO:tensorflow:Trainable Variables Total size: 2005632\n",
      "INFO:tensorflow:Using optimizer Adam\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-04-04 20:11:21.518234: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2019-04-04 20:11:40.608730: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2019-04-04 20:11:40.962026: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.324726, step = 1\n",
      "2019-04-04 20:11:54.030903: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "2019-04-04 20:11:54.375646: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:01.872743: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:02.270441: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:10.278859: W tensorflow/core/framework/allocator.cc:101] Allocation of 116770500 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:10.640326: W tensorflow/core/framework/allocator.cc:101] Allocation of 116770500 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:18.579086: W tensorflow/core/framework/allocator.cc:101] Allocation of 99078000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:18.879034: W tensorflow/core/framework/allocator.cc:101] Allocation of 99078000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:25.799935: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:26.115163: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:33.065537: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:33.418248: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:40.853006: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:41.224219: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:48.903752: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:49.235241: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:56.316942: W tensorflow/core/framework/allocator.cc:101] Allocation of 116770500 exceeds 10% of system memory.\n",
      "2019-04-04 20:12:56.698367: W tensorflow/core/framework/allocator.cc:101] Allocation of 116770500 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Saving checkpoints for 10 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.321105.\n",
      "INFO:tensorflow:Reading data files from gs://qwiklabs-gcp-fa6d1361244be914/poetry/subset/poetry_line_problem-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8425_128.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_8425_128.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8425_128.top\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-04-20:13:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-04-20:13:21\n",
      "INFO:tensorflow:Saving dict for global step 10: global_step = 10, loss = 9.617647, metrics-poetry_line_problem/targets/accuracy = 0.0013612851, metrics-poetry_line_problem/targets/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/targets/accuracy_top5 = 0.001905799, metrics-poetry_line_problem/targets/approx_bleu_score = 0.77766705, metrics-poetry_line_problem/targets/neg_log_perplexity = -9.644455, metrics-poetry_line_problem/targets/rouge_2_fscore = 0.81033087, metrics-poetry_line_problem/targets/rouge_L_fscore = 0.82779336\n",
      "INFO:tensorflow:Reading data files from gs://qwiklabs-gcp-fa6d1361244be914/poetry/subset/poetry_line_problem-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8425_128.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_8425_128.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8425_128.top\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-04-20:13:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-04-20:13:38\n",
      "INFO:tensorflow:Saving dict for global step 10: global_step = 10, loss = 9.617647, metrics-poetry_line_problem/targets/accuracy = 0.0013612851, metrics-poetry_line_problem/targets/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/targets/accuracy_top5 = 0.001905799, metrics-poetry_line_problem/targets/approx_bleu_score = 0.77766705, metrics-poetry_line_problem/targets/neg_log_perplexity = -9.644455, metrics-poetry_line_problem/targets/rouge_2_fscore = 0.81033087, metrics-poetry_line_problem/targets/rouge_L_fscore = 0.82779336\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=gs://${BUCKET}/poetry/subset\n",
    "OUTDIR=./trained_model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR --job-dir=$OUTDIR --train_steps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Train model locally on full dataset (use if running on Notebook Instance with a GPU)\n",
    "\n",
    "You can train on the full dataset if you are on a Google Cloud Notebook Instance with a P100 or better GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "LOCALGPU=\"--train_steps=7500 --worker_gpu=1 --hparams_set=transformer_poetry\"\n",
    "\n",
    "DATA_DIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR ${LOCALGPU}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Train on Cloud ML Engine\n",
    "\n",
    "tensor2tensor has a convenient --cloud_mlengine option to kick off the training on the managed service.\n",
    "It uses the [Python API](https://cloud.google.com/ml-engine/docs/training-jobs) mentioned in the Cloud ML Engine docs, rather than requiring you to use gcloud to submit the job.\n",
    "<p>\n",
    "Note: your project needs P100 quota in the region.\n",
    "<p>\n",
    "The echo is because t2t-trainer asks you to confirm before submitting the job to the cloud. Ignore any error about \"broken pipe\".\n",
    "If you see a message similar to this:\n",
    "<pre>\n",
    "    [... cloud_mlengine.py:392] Launched transformer_poetry_line_problem_t2t_20190323_000631. See console to track: https://console.cloud.google.com/mlengine/jobs/.\n",
    "</pre>\n",
    "then, this step has been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model us-east1 poetry_190404_201546\n",
      "Confirm (Y/n)? > "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "INFO:tensorflow:Launching job transformer_poetry_line_problem_t2t_20190404_201609 with ML Engine spec:\n",
      "{'trainingInput': {'jobDir': 'gs://qwiklabs-gcp-fa6d1361244be914/poetry/model', 'runtimeVersion': '1.9', 'scaleTier': 'CUSTOM', 'masterType': 'standard_p100', 'region': u'us-east1', 'args': ['--eval_steps=100', '--cloud_tpu=False', '--decode_hparams=', '--sync=False', '--eval_run_autoregressive=False', '--eval_use_test_set=False', '--worker_id=0', '--eval_early_stopping_metric_minimize=True', '--worker_replicas=1', '--worker_gpu_memory_fraction=0.95', '--train_steps=7500', '--cloud_tpu_name=None-tpu', '--locally_shard_to_cpu=False', '--iterations_per_loop=100', '--registry_help=False', '--eval_throttle_seconds=600', '--worker_gpu=1', '--keep_checkpoint_max=20', '--save_checkpoints_secs=0', '--gpu_order=', '--log_step_count_steps=100', '--master=', '--generate_data=False', '--intra_op_parallelism_threads=0', '--enable_graph_rewriter=False', '--eval_early_stopping_metric=loss', '--output_dir=gs://qwiklabs-gcp-fa6d1361244be914/poetry/model', '--profile=False', '--ps_job=/job:ps', '--tmp_dir=/tmp/t2t_datagen', '--schedule=continuous_train_and_eval', '--inter_op_parallelism_threads=0', '--hparams=', '--use_tpu=False', '--eval_early_stopping_metric_delta=0.1', '--ps_gpu=0', '--tfdbg=False', '--local_eval_frequency=1000', '--data_dir=gs://qwiklabs-gcp-fa6d1361244be914/poetry/subset', '--ps_replicas=0', '--use_tpu_estimator=False', '--export_saved_model=False', '--problem=poetry_line_problem', '--log_device_placement=False', '--hparams_set=transformer_poetry', '--dbgprofile=False', '--timit_paths=', '--cloud_skip_confirmation=False', '--cloud_delete_on_done=False', '--tpu_num_shards=8', '--cloud_vm_name=None-vm', '--xla_compile=False', '--parsing_path=', '--worker_job=/job:localhost', '--model=transformer', '--keep_checkpoint_every_n_hours=10000'], 'pythonModule': 'tensor2tensor.bin.t2t_trainer', 'pythonVersion': '2.7'}, 'jobId': 'transformer_poetry_line_problem_t2t_20190404_201609'}\n",
      "INFO:tensorflow:Tarring and pushing local Tensor2Tensor package.\n",
      "INFO:tensorflow:Found PyPI T2T installation. Launching tensor2tensor==1.8.0\n",
      "Copying file:///tmp/tensor2tensor_tmp.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  271.0 B]                                                \r",
      "/ [1 files][  271.0 B/  271.0 B]                                                \r\n",
      "Operation completed over 1 objects/271.0 B.                                      \n",
      "INFO:tensorflow:Tarring and pushing t2t_usr_dir.\n",
      "Copying file:///tmp/t2t_usr_container.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  2.7 KiB]                                                \r",
      "/ [1 files][  2.7 KiB/  2.7 KiB]                                                \r\n",
      "Operation completed over 1 objects/2.7 KiB.                                      \n",
      "INFO:tensorflow:Launched transformer_poetry_line_problem_t2t_20190404_201609. See console to track: https://console.cloud.google.com/mlengine/jobs/.\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "GPU=\"--train_steps=7500 --cloud_mlengine --worker_gpu=1 --hparams_set=transformer_poetry\"\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "yes Y | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  ${GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## CHANGE the job name (based on output above: You will see a line such as Launched transformer_poetry_line_problem_t2t_20190322_233159)\n",
    "gcloud ml-engine jobs describe transformer_poetry_line_problem_t2t_20190323_003001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job took about <b>25 minutes</b> for me and ended with these evaluation metrics:\n",
    "<pre>\n",
    "Saving dict for global step 8000: global_step = 8000, loss = 6.03338, metrics-poetry_line_problem/accuracy = 0.138544, metrics-poetry_line_problem/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/accuracy_top5 = 0.232037, metrics-poetry_line_problem/approx_bleu_score = 0.00492648, metrics-poetry_line_problem/neg_log_perplexity = -6.68994, metrics-poetry_line_problem/rouge_2_fscore = 0.00256089, metrics-poetry_line_problem/rouge_L_fscore = 0.128194\n",
    "</pre>\n",
    "Notice that accuracy_per_sequence is 0 -- Considering that we are asking the NN to be rather creative, that doesn't surprise me. Why am I looking at accuracy_per_sequence and not the other metrics? This is because it is more appropriate for problem we are solving; metrics like Bleu score are better for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Train on a directly-connected TPU\n",
    "\n",
    "If you are running on a VM connected directly to a Cloud TPU, you can run t2t-trainer directly. Unfortunately, you won't see any output from Jupyter while the program is running.\n",
    "\n",
    "Compare this command line to the one using GPU in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# use one of these\n",
    "TPU=\"--train_steps=7500 --use_tpu=True --cloud_tpu_name=laktpu --hparams_set=transformer_poetry_tpu\"\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_tpu\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  ${TPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model_tpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job took about <b>10 minutes</b> for me and ended with these evaluation metrics:\n",
    "<pre>\n",
    "Saving dict for global step 8000: global_step = 8000, loss = 6.03338, metrics-poetry_line_problem/accuracy = 0.138544, metrics-poetry_line_problem/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/accuracy_top5 = 0.232037, metrics-poetry_line_problem/approx_bleu_score = 0.00492648, metrics-poetry_line_problem/neg_log_perplexity = -6.68994, metrics-poetry_line_problem/rouge_2_fscore = 0.00256089, metrics-poetry_line_problem/rouge_L_fscore = 0.128194\n",
    "</pre>\n",
    "Notice that accuracy_per_sequence is 0 -- Considering that we are asking the NN to be rather creative, that doesn't surprise me. Why am I looking at accuracy_per_sequence and not the other metrics? This is because it is more appropriate for problem we are solving; metrics like Bleu score are better for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 4: Training longer\n",
    "\n",
    "Let's train on 4 GPUs for 75,000 steps. Note the change in the last line of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "XXX This takes 3 hours on 4 GPUs. Remove this line if you are sure you want to do this.\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_full2\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --train_steps=75000 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job took <b>12 hours</b> for me and ended with these metrics:\n",
    "<pre>\n",
    "global_step = 76000, loss = 4.99763, metrics-poetry_line_problem/accuracy = 0.219792, metrics-poetry_line_problem/accuracy_per_sequence = 0.0192308, metrics-poetry_line_problem/accuracy_top5 = 0.37618, metrics-poetry_line_problem/approx_bleu_score = 0.017955, metrics-poetry_line_problem/neg_log_perplexity = -5.38725, metrics-poetry_line_problem/rouge_2_fscore = 0.0325563, metrics-poetry_line_problem/rouge_L_fscore = 0.210618\n",
    "</pre>\n",
    "At least the accuracy per sequence is no longer zero. It is now 0.0192308 ... note that we are using a relatively small dataset (12K lines) and this is *tiny* in the world of natural language problems.\n",
    "<p>\n",
    "In order that you have your expectations set correctly: a high-performing translation model needs 400-million lines of input and takes 1 whole day on a TPU pod!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/checkpoint\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/events.out.tfevents.1554409195.cmle-training-13740022145287437184\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/flags.txt\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/flags_t2t.txt\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/graph.pbtxt\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/hparams.json\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-0.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-0.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-0.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-0.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-1000.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-1000.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-1000.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-1000.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-2000.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-2000.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-2000.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-2000.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-3000.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-3000.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-3000.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-3000.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-4000.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-4000.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-4000.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-4000.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-5000.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-5000.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-5000.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-5000.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-6000.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-6000.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-6000.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-6000.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7000.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7000.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7000.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7000.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7500.data-00000-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7500.data-00001-of-00002\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7500.index\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7500.meta\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/t2t_usr_container.tar.gz\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/tensor2tensor_tmp.tar.gz\n",
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/eval/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model   #_modeltpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-predict\n",
    "\n",
    "How will our poetry model do when faced with Rumi's spiritual couplets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/poetry/rumi.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/poetry/rumi.txt\n",
    "Where did the handsome beloved go?\n",
    "I wonder, where did that tall, shapely cypress tree go?\n",
    "He spread his light among us like a candle.\n",
    "Where did he go? So strange, where did he go without me?\n",
    "All day long my heart trembles like a leaf.\n",
    "All alone at midnight, where did that beloved go?\n",
    "Go to the road, and ask any passing traveler — \n",
    "That soul-stirring companion, where did he go?\n",
    "Go to the garden, and ask the gardener — \n",
    "That tall, shapely rose stem, where did he go?\n",
    "Go to the rooftop, and ask the watchman — \n",
    "That unique sultan, where did he go?\n",
    "Like a madman, I search in the meadows!\n",
    "That deer in the meadows, where did he go?\n",
    "My tearful eyes overflow like a river — \n",
    "That pearl in the vast sea, where did he go?\n",
    "All night long, I implore both moon and Venus — \n",
    "That lovely face, like a moon, where did he go?\n",
    "If he is mine, why is he with others?\n",
    "Since he’s not here, to what “there” did he go?\n",
    "If his heart and soul are joined with God,\n",
    "And he left this realm of earth and water, where did he go?\n",
    "Tell me clearly, Shams of Tabriz,\n",
    "Of whom it is said, “The sun never dies” — where did he go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the odd-numbered lines. We'll compare how close our model can get to the beauty of Rumi's second lines given his first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where did the handsome beloved go\n",
      "he spread his light among us like a candle\n",
      "all day long my heart trembles like a leaf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "awk 'NR % 2 == 1' data/poetry/rumi.txt | tr '[:upper:]' '[:lower:]' | sed \"s/[^a-z\\'-\\ ]//g\" > data/poetry/rumi_leads.txt\n",
    "head -3 data/poetry/rumi_leads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:198: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f62cf2a4810>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", 'use_tpu': False, '_tf_random_seed': None, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_train_distribute': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': 'gs://qwiklabs-gcp-fa6d1361244be914/poetry/model', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f62d0f6a790>, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7f62d83e4398>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:decode_hp.batch_size not specified; default=32\n",
      "INFO:tensorflow:Performing decoding from a file.\n",
      "INFO:tensorflow:Getting sorted inputs\n",
      "INFO:tensorflow: batch 1\n",
      "INFO:tensorflow:Decoding batch 0\n",
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'infer'\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Beam Decoding with beam size 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-04-04 20:41:20.653471: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Restoring parameters from gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference results INPUT: if his heart and soul are joined with god\n",
      "INFO:tensorflow:Inference results OUTPUT: and ready harbinger\n",
      "INFO:tensorflow:Inference results INPUT: if he is mine why is he with others\n",
      "INFO:tensorflow:Inference results OUTPUT: for those obstinate questionings\n",
      "INFO:tensorflow:Inference results INPUT: all night long i implore both moon and venus\n",
      "INFO:tensorflow:Inference results OUTPUT: remark did ill confine\n",
      "INFO:tensorflow:Inference results INPUT: go to the road and ask any passing traveler\n",
      "INFO:tensorflow:Inference results OUTPUT: and ilka bird and ilka bird\n",
      "INFO:tensorflow:Inference results INPUT: all day long my heart trembles like a leaf\n",
      "INFO:tensorflow:Inference results OUTPUT: i thank whatever gods were ruth\n",
      "INFO:tensorflow:Inference results INPUT: he spread his light among us like a candle\n",
      "INFO:tensorflow:Inference results OUTPUT: and cream of chariot-flame\n",
      "INFO:tensorflow:Inference results INPUT: like a madman i search in the meadows\n",
      "INFO:tensorflow:Inference results OUTPUT: and like a staff\n",
      "INFO:tensorflow:Inference results INPUT: go to the rooftop and ask the watchman\n",
      "INFO:tensorflow:Inference results OUTPUT: and perfidlest perfidious bark\n",
      "INFO:tensorflow:Inference results INPUT: go to the garden and ask the gardener\n",
      "INFO:tensorflow:Inference results OUTPUT: the public deity\n",
      "INFO:tensorflow:Inference results INPUT: my tearful eyes overflow like a river\n",
      "INFO:tensorflow:Inference results OUTPUT: the woods among the woods\n",
      "INFO:tensorflow:Inference results INPUT: tell me clearly shams of tabriz\n",
      "INFO:tensorflow:Inference results OUTPUT: and gorgeous dames and gorgeous turrets\n",
      "INFO:tensorflow:Inference results INPUT: where did the handsome beloved go\n",
      "INFO:tensorflow:Inference results OUTPUT: the idol of gold\n",
      "INFO:tensorflow:Elapsed Time: 11.36457\n",
      "INFO:tensorflow:Averaged Single Token Generation Time: 0.0171226\n",
      "INFO:tensorflow:Writing decodes into data/poetry/rumi_leads.txt.transformer.transformer_poetry.poetry_line_problem.beam4.alpha0.6.decodes\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# same as the above training job ...\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model #_tpu  # or ${TOPDIR}/poetry/model_full\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry #_tpu\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note </b> if you get an error about \"AttributeError: 'HParams' object has no attribute 'problems'\" please <b>Reset Session</b>, run the cell that defines the PROBLEM and run the above cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the idol of gold\n",
      "and cream of chariot-flame\n",
      "i thank whatever gods were ruth\n",
      "and ilka bird and ilka bird\n",
      "the public deity\n",
      "and perfidlest perfidious bark\n",
      "and like a staff\n",
      "the woods among the woods\n",
      "remark did ill confine\n",
      "for those obstinate questionings\n",
      "and ready harbinger\n",
      "and gorgeous dames and gorgeous turrets\n"
     ]
    }
   ],
   "source": [
    "%%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are still phrases and not complete sentences. This indicates that we might need to train longer or better somehow. We need to diagnose the model ...\n",
    "<p>\n",
    "    \n",
    "### Diagnosing training run\n",
    "\n",
    "<p>\n",
    "Let's diagnose the training run to see what we'd improve the next time around.\n",
    "(Note that this package may not be present on Jupyter -- `pip install pydatalab` if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 6807. Click <a href=\"/_proxy/47653/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6807"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/poetry/model_full'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped TensorBoard with pid 6807\n"
     ]
    }
   ],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"diagrams/poetry_loss.png\"/></td>\n",
    "<td><img src=\"diagrams/poetry_acc.png\"/></td>\n",
    "</table>\n",
    "Looking at the loss curve, it is clear that we are overfitting (note that the orange training curve is well below the blue eval curve). Both loss curves and the accuracy-per-sequence curve, which is our key evaluation measure, plateaus after 40k. (The red curve is a faster way of computing the evaluation metric, and can be ignored). So, how do we improve the model? Well, we need to reduce overfitting and make sure the eval metrics keep going down as long as the loss is also going down.\n",
    "<p>\n",
    "What we really need to do is to get more data, but if that's not an option, we could try to reduce the NN and increase the dropout regularization. We could also do hyperparameter tuning on the dropout and network sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "tensor2tensor also supports hyperparameter tuning on Cloud ML Engine. Note the addition of the autotune flags.\n",
    "<p>\n",
    "The `transformer_poetry_range` was registered in problem.py above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "XXX This takes about 15 hours and consumes about 420 ML units.  Uncomment if you wish to proceed anyway\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_hparam\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --hparams_range=transformer_poetry_range \\\n",
    "  --autotune_objective='metrics-poetry_line_problem/accuracy_per_sequence' \\\n",
    "  --autotune_maximize \\\n",
    "  --autotune_max_trials=4 \\\n",
    "  --autotune_parallel_trials=4 \\\n",
    "  --train_steps=7500 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above job, it took about 15 hours and finished with these as the best parameters:\n",
    "<pre>\n",
    "{\n",
    "      \"trialId\": \"37\",\n",
    "      \"hyperparameters\": {\n",
    "        \"hp_num_hidden_layers\": \"4\",\n",
    "        \"hp_learning_rate\": \"0.026711152525921437\",\n",
    "        \"hp_hidden_size\": \"512\",\n",
    "        \"hp_attention_dropout\": \"0.60589466163419292\"\n",
    "      },\n",
    "      \"finalMetric\": {\n",
    "        \"trainingStep\": \"8000\",\n",
    "        \"objectiveValue\": 0.0276162791997\n",
    "      }\n",
    "</pre>\n",
    "In other words, the accuracy per sequence achieved was 0.027 (as compared to 0.019 before hyperparameter tuning, so a <b>40% improvement!</b>) using 4 hidden layers, a learning rate of 0.0267, a hidden size of 512 and droput probability of 0.606. This is inspite of training for only 7500 steps instead of 75,000 steps ... we could train for 75k steps with these parameters, but I'll leave that as an exercise for you.\n",
    "<p>\n",
    "Instead, let's try predicting with this optimized model. Note the addition of the hp* flags in order to override the values hardcoded in the source code. (there is no need to specify learning rate and dropout because they are not used during inference). I am using 37 because I got the best result at trialId=37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# same as the above training job ...\n",
    "BEST_TRIAL=28  # CHANGE as needed.\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_hparam/$BEST_TRIAL\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE \\\n",
    "  --hparams=\"num_hidden_layers=4,hidden_size=512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first three line. I'm showing the first line of the couplet provided to the model, how the AI model that we trained complets it and how Rumi completes it:\n",
    "<p>\n",
    "INPUT: where did the handsome beloved go <br/>\n",
    "AI: where art thou worse to me than dead <br/>\n",
    "RUMI: I wonder, where did that tall, shapely cypress tree go?\n",
    "<p>\n",
    "INPUT: he spread his light among us like a candle <br/>\n",
    "AI: like the hurricane eclipse <br/>\n",
    "RUMI: Where did he go? So strange, where did he go without me? <br/>\n",
    "<p>\n",
    "INPUT: all day long my heart trembles like a leaf <br/>\n",
    "AI: and through their hollow aisles it plays <br/>\n",
    "RUMI: All alone at midnight, where did that beloved go? \n",
    "<p>\n",
    "Oh wow. The couplets as completed are quite decent considering that:\n",
    "* We trained the model on American poetry, so feeding it Rumi is a bit out of left field.\n",
    "* Rumi, of course, has a context and thread running through his lines while the AI (since it was fed only that one line) doesn't. \n",
    "\n",
    "<p>\n",
    "\"Spreading light like a hurricane eclipse\" is a metaphor I won't soon forget. And it was created by a machine learning model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving poetry\n",
    "\n",
    "How would you serve these predictions? There are two ways:\n",
    "<ol>\n",
    "<li> Use [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/deploying-models) -- this is serverless and you don't have to manage any infrastructure.\n",
    "<li> Use [Kubeflow](https://github.com/kubeflow/kubeflow/blob/master/user_guide.md) on Google Kubernetes Engine -- this uses clusters but will also work on-prem on your own Kubernetes cluster.\n",
    "</ol>\n",
    "<p>\n",
    "In either case, you need to export the model first and have TensorFlow serving serve the model. The model, however, expects to see *encoded* (i.e. preprocessed) data. So, we'll do that in the Python Flask application (in AppEngine Flex) that serves the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:198: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "INFO:tensorflow:datashard_devices: ['']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd35ab68090>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", 'use_tpu': False, '_tf_random_seed': None, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_train_distribute': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': 'gs://qwiklabs-gcp-fa6d1361244be914/poetry/model', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fd361eed7d0>, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7fd35d7aac08>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:563: make_export_strategy (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:483: __new__ (from tensorflow.contrib.learn.python.learn.export_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate, and use tf.estimator.Exporter.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'infer'\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Beam Decoding with beam size 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "2019-04-04 20:58:39.917675: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Restoring parameters from gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/model.ckpt-7500\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/export/Servo/temp-1554411519/saved_model.pbtxt\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:480: garbage_collect_exports (from tensorflow.contrib.learn.python.learn.utils.saved_model_export_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Switch to tf.estimator.Exporter and associated utilities.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:393: largest_export_versions (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file management or use Saver.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:394: negation (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file management or use Saver.\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py:396: get_paths (from tensorflow.contrib.learn.python.learn.utils.gc) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please implement your own file name management.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-exporter \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/export/Servo/1554411519/\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['input'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serialized_example:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['batch_prediction_key'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 1)\n",
      "      name: DatasetToSingleElement:0\n",
      "  outputs['outputs'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, -1)\n",
      "      name: transformer/strided_slice_10:0\n",
      "  outputs['scores'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: transformer/strided_slice_11:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model/export/Servo | tail -1)\n",
    "echo $MODEL_LOCATION\n",
    "saved_model_cli show --dir $MODEL_LOCATION --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mlengine.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile mlengine.json\n",
    "description: Poetry service on ML Engine\n",
    "autoScaling:\n",
    "    minNodes: 1  # We don't want this model to autoscale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and deploying poetry v1 from gs://qwiklabs-gcp-fa6d1361244be914/poetry/model/export/Servo/1554411519/ ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This will delete version [v1]...\n",
      "\n",
      "Do you want to continue (Y/n)?  Please enter 'y' or 'n':  Please enter 'y' or 'n':  Please enter 'y' or 'n':  Please enter 'y' or 'n':  \n",
      "ERROR: (gcloud.ml-engine.versions.delete) NOT_FOUND: Field: name Error: The model resource: \"poetry\" was not found. Please create the Cloud ML model resource first by using 'gcloud ml-engine models create poetry'.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: \"The model resource: \\\"poetry\\\" was not found. Please create the\\\n",
      "      \\ Cloud ML model resource first by using 'gcloud ml-engine models create poetry'.\"\n",
      "    field: name\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"poetry\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model/export/Servo | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "#gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud alpha ml-engine versions create --machine-type=mls1-highcpu-4 ${MODEL_VERSION} \\\n",
    "       --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=1.5 --config=mlengine.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your current Cloud SDK version is: 229.0.0\n",
      "You will be upgraded to version: 241.0.0\n",
      "\n",
      "+---------------------------------------------------------+\n",
      "|            These components will be updated.            |\n",
      "+---------------------------------+------------+----------+\n",
      "|               Name              |  Version   |   Size   |\n",
      "+---------------------------------+------------+----------+\n",
      "| BigQuery Command Line Tool      |     2.0.43 |  < 1 MiB |\n",
      "| Cloud SDK Core Libraries        | 2019.04.02 | 10.1 MiB |\n",
      "| Cloud Storage Command Line Tool |       4.38 |  3.8 MiB |\n",
      "| gcloud Alpha Commands           | 2019.02.22 |  < 1 MiB |\n",
      "| gcloud Beta Commands            | 2019.02.22 |  < 1 MiB |\n",
      "| gcloud cli dependencies         | 2019.03.17 |  2.4 MiB |\n",
      "+---------------------------------+------------+----------+\n",
      "\n",
      "The following release notes are new in this upgrade.\n",
      "Please read carefully for information about new features, breaking changes,\n",
      "and bugs fixed.  The latest full release notes can be viewed at:\n",
      "  https://cloud.google.com/sdk/release_notes\n",
      "\n",
      "241.0.0 (2019-04-02)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Functions)** Modified gcloud functions deploy such that the\n",
      "        --runtime flag needs to be set when deploying a new function.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Java SDK to version 1.9.73. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "      o Updated the Python SDK to version 1.9.85. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "  BigQuery\n",
      "      o Added DML/DDL query results that display the number of affected rows\n",
      "        (for DML) and performed operation and target name (for DDL).\n",
      "\n",
      "  Cloud Asset Inventory\n",
      "      o Promoted gcloud asset command group to GA\n",
      "\n",
      "  Cloud Bigtable\n",
      "      o Promoted the following to GA:\n",
      "        * gcloud bigtable app-profiles command group\n",
      "        * gcloud bigtable clusters create\n",
      "        * gcloud bigtable clusters update\n",
      "        * gcloud bigtable clusters delete\n",
      "\n",
      "  Cloud Build\n",
      "      o Added --network=cloudbuild to gcloud builds submit --tag invocations\n",
      "        of docker build. This enables access to metadata during Dockerfile RUN\n",
      "        operations at build time.\n",
      "\n",
      "  Cloud Composer\n",
      "      o Added three new flags to gcloud beta composer environments create to\n",
      "        support Private IP Composer environments:\n",
      "        * --enable-private-environment\n",
      "        * --enable-private-endpoint\n",
      "        * --master-ipv4-cidr\n",
      "      o Added gcloud beta composer environments list-upgrades to list all\n",
      "        image version upgrades that are supported for a specified environment.\n",
      "      o Added two mutually exclusive flags to gcloud beta composer\n",
      "        environments update to allow for in-place environment upgrades:\n",
      "        * --airflow-version\n",
      "        * --image-version\n",
      "\n",
      "  Cloud DNS\n",
      "      o Added support for DNS peering in gcloud beta dns managed-zones.\n",
      "      o Added --enable-logging flag to gcloud beta dns policies to enable\n",
      "        query logging.\n",
      "\n",
      "  Cloud Data Catalog\n",
      "      o Added the gcloud beta data-catalog entries command group, which\n",
      "        provides lookup, describe, and schema update functionality for Cloud\n",
      "        Data Catalog entries.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.4.2\n",
      "        * Adds implementations for BeginTransaction and Rollback\n",
      "\n",
      "  Cloud Functions\n",
      "      o Added --service-account flag to gcloud functions deploy.\n",
      "      o Added --vpc-connector flag to gcloud beta functions deploy.\n",
      "\n",
      "  Cloud Memorystore\n",
      "      o Added gcloud redis instances failover which provides the ability to\n",
      "        failover a standard tier Cloud Memorystore for Redis instance from the\n",
      "        master node to its replica.\n",
      "      o Added --redis-version flag to gcloud beta redis instances create to\n",
      "        enable the specification of a preferred Redis version compatibility;\n",
      "        this can be either redis_3_2 or redis_4_0.\n",
      "      o Modified the --update-redis-config flag of gcloud redis instances\n",
      "        update to accept three additional parameters for Redis 4.0 compatible\n",
      "        instances: activedefrag, lfu-decay-time, lfu-log-factor.\n",
      "\n",
      "  Cloud PubSub\n",
      "      o Added optional flags --push-auth-service-account and\n",
      "        --push-auth-token-audience for defining an authenticated push\n",
      "        subscription in gcloud beta pubsub subscriptions\n",
      "        <create|update|modify-push-config>.\n",
      "\n",
      "  Cloud Storage\n",
      "    *\n",
      "      o Updated gsutil component to 4.38.\n",
      "\n",
      "  Cloud Video Intelligence API\n",
      "      o Added gcloud beta ml video\n",
      "        transcribe-speech|detect-text|detect-object commands.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --logging-aggregation-interval, --logging-flow-sampling, and\n",
      "        --logging-metadata flags of gcloud compute networks subnets\n",
      "        <create|update> to beta.\n",
      "      o Modified gcloud compute networks subnets update to support specifying\n",
      "        --logging-aggregation-interval, --logging-flow-sampling, and\n",
      "        --logging-metadata flags in a single call.\n",
      "      o Promoted gcloud compute reservations command group to beta.\n",
      "      o Promoted --reservation and --reservation-affinity of gcloud compute\n",
      "        instance-templates create to beta.\n",
      "      o Promoted --reservation and --reservations-from-file of gcloud compute\n",
      "        commitments create to beta.\n",
      "      o Promoted gcloud compute commitments update-reservations to beta.\n",
      "      o Promoted 100G interconnect link type support for gcloud compute\n",
      "        interconnects create to beta.\n",
      "      o Deprecated the creation of new legacy network.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Promoted --num-flaky-test-attempts flag of gcloud firebase test\n",
      "        <android|ios> run to GA. This flag specifies how many times to rerun\n",
      "        any failed executions.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --security-group flag of gcloud container clusters\n",
      "        create to beta. The flag enables support for Google Groups in\n",
      "        Kubernetes RBAC rules.\n",
      "      o Added the --enable-intra-node-visibility flag to gcloud beta\n",
      "        container clusters create.\n",
      "      o Promoted the --enable-tpu flag and the --tpu-ipv4-cidr flag of gcloud\n",
      "        container clusters create to GA. The flags enables support for using\n",
      "        Cloud TPU in Google Kubernetes Engine clusters.\n",
      "      o Changed the default output formatting for the gcloud beta container\n",
      "        binauthz attestations list command.\n",
      "      o Google Kubernetes Engine kubectl is updated to 1.11.9. Addresses\n",
      "        security vulnerability: CVE-2019-1002101.\n",
      "      o Updated extra Google Kubernetes Engine kubectl versions:\n",
      "        * kubectl.1.11 (patch 1.11.9)\n",
      "        * kubectl.1.12 (patch 1.12.7)\n",
      "        * kubectl.1.13 (patch 1.13.5)\n",
      "        * kubectl.1.14 (patch 1.14.0)\n",
      "      o Removed extra Google Kubernetes Engine kubectl versions, since these\n",
      "        versions are vulnerable.\n",
      "        * kubectl.1.9\n",
      "        * kubectl.1.10\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "240.0.0 (2019-03-26)\n",
      "  Breaking Changes\n",
      "      o **(Kubernetes Engine)** Enabled node auto-upgrade by default for\n",
      "        clusters and node-pools created with gcloud beta container\n",
      "        <clusters|node-pools> create. To disable manually, **(Kubernetes\n",
      "        Engine)** use the --no-enable-autoupgrade flag.\n",
      "\n",
      "  Cloud SDK\n",
      "      o Added the --impersonate-service-account flag to gcloud.\n",
      "\n",
      "  App Engine\n",
      "      o Fixed a bug which could cause gcloud to incorrectly print http URLs\n",
      "        for services, when https URLs would be more appropriate.\n",
      "      o Removed 32MB file size limit for second generation runtimes.\n",
      "\n",
      "  Cloud DNS\n",
      "      o Promoted private zones of the gcloud dns managed-zones command group\n",
      "        to GA. Use the --visibility and --networks flags to configure zone\n",
      "        visibility.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added gcloud beta dataproc <jobs submit|workflow-templates add-job>\n",
      "        presto to enable submitting Presto jobs to a Dataproc cluster and\n",
      "        adding Presto jobs to workflow templates, respectively.\n",
      "      o Added --enable-component-gateway flag to gcloud beta dataproc\n",
      "        clusters create and gcloud beta dataproc workflow-templates\n",
      "        set-managed-cluster.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --failover-ratio, --drop-traffic-if-unhealthy, and\n",
      "        --connection-drain-on-failover flags of gcloud compute backend-services\n",
      "        <create|update> to beta.\n",
      "      o Promoted --failover flag of gcloud compute backend-services\n",
      "        <add_backend|update_backend> to beta.\n",
      "      o Promoted gcloud compute instance-groups managed wait-until command to\n",
      "        beta.\n",
      "      o Promoted --region flag of gcloud compute disks and related commands\n",
      "        to GA.\n",
      "      o Added gcloud beta compute external-vpn-gateway command group to\n",
      "        enable reading and manipulating of Compute Engine external VPN\n",
      "        gateways.\n",
      "      o Added gcloud beta compute vpn-gateway command group to enable reading\n",
      "        and manipulating of Compute Engine VPN gateways.\n",
      "      o Updated gcloud beta compute vpn-tunnel command to enable the creation\n",
      "        of HA VPN tunnels.\n",
      "\n",
      "  Identity and Access Management\n",
      "      o Added a new column, DISABLED, to the return table of gcloud iam\n",
      "        service-account list, which displays the state of the service account\n",
      "        listed.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added the --release-channel flag to gcloud alpha container clusters\n",
      "        create for subscribing a cluster to a release channel.\n",
      "      o Promoted --default-max-pods-per-node flag of gcloud container\n",
      "        clusters create from Beta to GA.\n",
      "      o Promoted --max-pods-per-node flag of gcloud container node-pools\n",
      "        create from Beta to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "239.0.0 (2019-03-19)\n",
      "  Cloud Dataproc\n",
      "      o Added gcloud beta dataproc autoscaling-policies command group for\n",
      "        managing Cloud Dataproc autoscaling policies. For more information,\n",
      "        see:\n",
      "        https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling\n",
      "      o Added --autoscaling-policy flag to gcloud beta dataproc clusters\n",
      "        <create|update> and gcloud beta dataproc workflow-templates\n",
      "        set-managed-cluster to support enabling and disabling autoscaling on\n",
      "        Cloud Dataproc clusters with autoscaling policies.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.4.1\n",
      "        * Fixes bug where an empty CommitRequest was trigger exceptions\n",
      "\n",
      "  Cloud Resource Manager\n",
      "      o Promoted resource-manager folders command group to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --bandwidth flag of gcloud compute interconnects attachments\n",
      "        dedicated <create|update> to GA.\n",
      "      o Promoted load_balancing_scheme enum value INTERNAL_SELF_MANAGED to\n",
      "        beta in 'gcloud compute forwarding-rules create' and 'gcloud compute\n",
      "        backend-services create'.\n",
      "      o Promoted gcloud compute instances get-shielded-identity to GA.\n",
      "      o Promoted the --shielded-vtpm, --shielded-secure-boot, and\n",
      "        --shielded-integrity-monitoring flags of gcloud compute instances\n",
      "        create to GA.\n",
      "      o Promoted the --shielded-vtpm, --shielded-secure-boot, and\n",
      "        --shielded-integrity-monitoring flags of gcloud compute\n",
      "        instance-templates create to GA.\n",
      "      o Promoted the --shielded-vtpm, --shielded-secure-boot,\n",
      "        --shielded-integrity-monitoring, and --shielded-learn-integrity-policy\n",
      "        flags of gcloud compute instances update to GA.\n",
      "      o Removed the deprecated --shielded-vm-vtpm, --shielded-vm-secure-boot,\n",
      "        and --shielded-vm-integrity-monitoring flags of gcloud compute\n",
      "        instances create from alpha and beta.\n",
      "      o Removed the deprecated --shielded-vm-vtpm, --shielded-vm-secure-boot,\n",
      "        and --shielded-vm-integrity-monitoring flags of gcloud compute\n",
      "        instance-templates create from alpha and beta.\n",
      "      o Removed the deprecated --shielded-vm-vtpm, --shielded-vm-secure-boot,\n",
      "        --shielded-vm-integrity-monitoring, and\n",
      "        --shielded-vm-learn-integrity-policy flags of gcloud compute instances\n",
      "        update from alpha and beta.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Update Google Container Engine's kubectl so that it now first\n",
      "        attempts\n",
      "    to match the API server version. If successful, the kubectl will delegate\n",
      "    to the matching version of kubectl. Otherwise, it defaults to execute the\n",
      "    current 1.11.7 version of kubectl.\n",
      "      o Updated extra Google Kubernetes Engine kubectl versions:\n",
      "        * kubectl.1.9 (patch 1.9.11)\n",
      "        * kubectl.1.10 (patch 1.10.13)\n",
      "        * kubectl.1.11 (patch 1.11.8)\n",
      "        * kubectl.1.12 (patch 1.12.6)\n",
      "        * kubectl.1.13 (patch 1.13.4)\n",
      "\n",
      "      o In June 2019, node auto-upgrade will be enabled by default for newly\n",
      "        created\n",
      "    clusters and node pools. To disable it, use the --no-enable-autoupgrade\n",
      "    flag.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "238.0.0 (2019-03-12)\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.84. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "      o Added a new field, 'main', to app.yaml. This allows users of the Go\n",
      "        second-generation runtimes to specify which package to build. See\n",
      "        https://cloud.google.com/appengine/docs/standard/go111/config/appref\n",
      "        for more information.\n",
      "\n",
      "  App Engine Flexible Environment\n",
      "      o Promoted network.session_affinity flag in yaml file to GA.\n",
      "\n",
      "  Cloud Access Context Manager\n",
      "      o Promoted the gcloud access-context-manager command group to GA\n",
      "      o Removed the unrestricted-services field from alpha and beta, always\n",
      "        set to default ''.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Promoted --optional-components of gcloud dataproc clusters to GA.\n",
      "\n",
      "  Cloud Functions\n",
      "      o Promoted max-instances and clear-max-instances flags of gcloud\n",
      "        functions deploy to beta.\n",
      "\n",
      "  Cloud Machine Learning Engine\n",
      "      o Promoted --machine-type flag of gcloud ml-engine versions create\n",
      "        command to GA.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added the --enable-shielded-containers flag to gcloud beta container\n",
      "        clusters create.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "237.0.0 (2019-03-05)\n",
      "  Cloud SDK\n",
      "      o Fixed issue affecting users overwriting an existing Cloud SDK install\n",
      "        via the Windows installer, in which certain gcloud commands crashed\n",
      "        with a \"gcloud crashed (LayoutException): Multiple definitions for\n",
      "        release track\" error. This issue can be tracked at\n",
      "        <https://issuetracker.google.com/123390310>.\n",
      "\n",
      "  BigQuery\n",
      "      o Fixes bug in formatting pre-1900 timestamps.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Fixed a bug where several fields were hidden from gcloud dataproc\n",
      "        clusters\n",
      "    <import|export> in all release tracks. Note that these fields were always\n",
      "    available in gcloud dataproc clusters <create-from-file|describe>.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.37.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted the --server-binding flag of gcloud compute sole-tenancy\n",
      "        node-templates create to beta.\n",
      "      o Reduced lower bound for data disk sizes from 10GB to 1GB for gcloud\n",
      "        compute instances create.\n",
      "      o Added resources-accelerator to gcloud beta compute commitments\n",
      "        create.\n",
      "      o Promoted all option of --ports flag for gcloud compute\n",
      "        forwarding-rules to GA.\n",
      "      o Added INSTANCE_TEMPLATE and VERSION_NAME columns to output of gcloud\n",
      "        compute instance-groups managed list-instances.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Updated Google Kubernetes Engine's kubectl from version 1.10.7 to\n",
      "        1.11.7.\n",
      "      o Added extra Google Kubernetes Engine kubectl versions:\n",
      "        * kubectl.1.9\n",
      "        * kubectl.1.10\n",
      "        * kubectl.1.11\n",
      "        * kubectl.1.12\n",
      "        * kubectl.1.13\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "236.0.0 (2019-02-26)\n",
      "  Cloud SDK\n",
      "      o Modified error handling for gcloud auth revoke when revoking a\n",
      "        service account token to print a friendly error message with more\n",
      "        detailed instructions on how to revoke the token.\n",
      "\n",
      "  Cloud Asset Inventory\n",
      "      o Added --folder flag to gcloud beta asset export command.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added --kerberos-root-principal-password-uri, --kerberos-kms-key, and\n",
      "        --kerberos-config-file flags to gcloud beta dataproc clusters create\n",
      "        and gcloud beta dataproc workflow-templates set-managed-cluster.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Released Cloud Firestore Emulator version 1.4.0:\n",
      "        * Added support for the debug(...) function in security rules.\n",
      "        * Simplified security rule evaluation and rule coverage reports.\n",
      "\n",
      "  Cloud Services\n",
      "      o Promoted services vpc-peerings command group to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Removed auto-create-routes column from default output of gcloud beta\n",
      "        compute networks peerings list.\n",
      "      o Promoted --enable-logging and --log-filter flags of gcloud compute\n",
      "        routers nats to beta.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "235.0.0 (2019-02-19)\n",
      "  App Engine\n",
      "      o Fixed bug where deleting a service at the same time as deleting an\n",
      "        app version in an unrelated service caused deletion of the version to\n",
      "        fail.\n",
      "      o Updated the Python SDK to version 1.9.83. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "      o Updated the Java SDK to version 1.9.72. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "\n",
      "  Cloud Composer\n",
      "      o Promoted --airflow-version and --image-version flags of gcloud\n",
      "        composer environments create to GA. These mutually exclusive flags can\n",
      "        be used to specify the airflow version or image version used within a\n",
      "        created environment.\n",
      "\n",
      "  Cloud IoT\n",
      "      o Promoted gcloud iot devices gateways command group to GA.\n",
      "\n",
      "  Cloud Key Management Service\n",
      "      o Exposed a new format of Cavium's attestation introduced in Cavium's\n",
      "        new 3.2-08 version.\n",
      "\n",
      "  Compute Engine\n",
      "      o Updated the gcloud beta compute start-iap-tunnel command for Cloud\n",
      "        IAP TCP Forwarding to listen on both IPv4 and IPv6 for localhost.\n",
      "      o Updated Windows PuTTY executables to 0.70.\n",
      "      o Promoted gcloud compute networks peerings list-routes command to\n",
      "        beta.\n",
      "      o Modified the output of gcloud beta compute networks peerings list to\n",
      "        include IMPORT_CUSTOM_ROUTES/EXPORT_CUSTOM_ROUTES columns.\n",
      "      o Promoted --resource-policies of gcloud compute disks create to beta.\n",
      "      o Promoted --enable-logging and --logging-sample-rate flags of gcloud\n",
      "        compute backend-services <create|update> to beta.\n",
      "      o Promoted --bandwidth flag of gcloud compute interconnects attachments\n",
      "        dedicated <create|update> to beta\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Added an ignore: option to the --robo-directives flag of gcloud beta\n",
      "        firebase test android run command. This option directs Robo to avoid\n",
      "        interactions with a user-defined UI element.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "234.0.0 (2019-02-12)\n",
      "  Cloud SDK\n",
      "      o Added the accessibility/screen_reader property. This property changes\n",
      "        some gcloud UX to make output more screen reader friendly. See gcloud\n",
      "        topic accessibility for more information.\n",
      "        * Accessibility support is still in the early stages, so please\n",
      "          report any issues that you would like fixed using gcloud feedback.\n",
      "\n",
      "  Cloud Asset Inventory\n",
      "      o Added gcloud beta asset command group to manage the Cloud Asset\n",
      "        Inventory.\n",
      "\n",
      "  Cloud Resource Manager\n",
      "      o Promoted resource-manager folders command group to beta\n",
      "\n",
      "  Cloud Services\n",
      "      o Added gcloud beta services vpc-peerings to support updating a\n",
      "        connection.\n",
      "\n",
      "  Cloud Source Repositories\n",
      "      o Promoted gcloud source project-configs command group to GA.\n",
      "      o Promoted gcloud source repos update to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted gcloud beta compute networks subnets <get|set>-iam-policy\n",
      "        and <add|remove>-iam-policy-bindings to GA.\n",
      "      o Promoted the following commands to beta:\n",
      "        * gcloud compute disks <add|remove-resource-policies>\n",
      "        * gcloud compute resource-policies create-snapshot-schedule\n",
      "        * gcloud compute resource-policies delete\n",
      "        * gcloud compute resource-policies describe\n",
      "        * gcloud compute resource-policies list\n",
      "      o Promoted --service-label flag of gcloud compute forwarding-rules\n",
      "        create to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "233.0.0 (2019-02-05)\n",
      "  Cloud Dataflow\n",
      "      o Added numWorkers, network, subnetwork and workerMachineType flags to\n",
      "    'gcloud beta dataflow jobs run' command\n",
      "\n",
      "  Cloud Datalab\n",
      "      o Updated the datalab component to the 20190116 release. Released\n",
      "        changes are documented in its tracking issue at\n",
      "        https://github.com/googledatalab/datalab/issues/2114\n",
      "        (https://github.com/googledatalab/datalab/issues/2114).\n",
      "\n",
      "  Cloud Filestore\n",
      "      o Promoted gcloud filestore command group to GA.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.3.0\n",
      "        * Added a clearDatabase RPC to delete all data in a database\n",
      "        * Added logging to assist with FIRESTORE_EMULATOR_HOST environment\n",
      "          variable\n",
      "        * The getDocument RPC now supports a read_time consistency selector\n",
      "        * Fixed bug related to rule evaluation callbacks\n",
      "\n",
      "  Cloud Machine Learning Engine\n",
      "      o Added support for custom server configuration to ml-engine jobs\n",
      "        submit training in beta. Added the following flags:\n",
      "        * --master-machine-type\n",
      "        * --master-accelerator\n",
      "        * --master-image-uri\n",
      "        * --worker-machine-type\n",
      "        * --worker-count\n",
      "        * --worker-accelerator\n",
      "        * --worker-image-uri\n",
      "        * --parameter-server-machine-type\n",
      "        * --parameter-server-count\n",
      "        * --parameter-server-accelerator\n",
      "        * --parameter-server-image-uri\n",
      "\n",
      "  Cloud Pub/Sub\n",
      "      o Promoted Snapshot & Seek features to GA. These features allow users\n",
      "        to create snapshots of subscription backlog state, and later restore\n",
      "        that state.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed gcloud sql connect whitelisting issues that resulted from\n",
      "        invalid datetime formatting.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.36.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted the --force-attach flag of compute instances attach-disk to\n",
      "        GA\n",
      "      o Added <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        gcloud beta compute networks subnets\n",
      "      o Promoted gcloud compute instances get-shielded-identity to beta.\n",
      "      o Promoted gcloud compute instance-groups managed update to GA together\n",
      "        with --health-check, --initial-delay and --clear-autohealing flags.\n",
      "      o Promoted --initial-delay and --health-check flags of gcloud compute\n",
      "        instance-groups managed create to GA.\n",
      "      o Enabled the use of multiple --network-interface flags with gcloud\n",
      "        compute <instances|instance-templates> create-with-container to support\n",
      "        using multiple network interfaces.\n",
      "      o Promoted gcloud compute instance-groups managed rolling-action\n",
      "        command group to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "232.0.0 (2019-01-29)\n",
      "  Breaking Changes\n",
      "      o **(Kubernetes Engine)** Added a warning on cluster and node-pool\n",
      "        creation to notify users that modifications on the boot disks of node\n",
      "        VMs do not persist across node recreations and must be done using a\n",
      "        DaemonSet.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Rolled back fix to gcloud sql connect that seems to be causing\n",
      "        additional issues connecting.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        GA in the following command groups:\n",
      "        * gcloud compute disks\n",
      "        * gcloud compute images\n",
      "        * gcloud compute instance-templates\n",
      "        * gcloud compute snapshots\n",
      "      o Added '--enable-display-device' to gcloud beta compute instances\n",
      "        <create|update>\n",
      "      o Deprecated gcloud compute instance-groups managed set-autohealing\n",
      "        command. Use gcloud compute instance-groups managed update instead.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --database-encryption-key flag of gcloud container\n",
      "        clusters\n",
      "    create to beta. The flag enables support for encryption of Kubernetes\n",
      "    Secrets.\n",
      "      o Modified the --enable-stackdriver-kubernetes flag to be a hard\n",
      "        requirement\n",
      "    for --addons=CloudRun. The CloudRun-on-GKE add-on depends on Stackdriver\n",
      "    Kubernetes Monitoring to enrich Kubernetes metadata for logs and metrics.\n",
      "      o Add --max-pods-per-node for gcloud beta container clusters create.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "231.0.0 (2019-01-23)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Updated the error messaging associated with failed\n",
      "        long-running operations.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.82. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/standard/python/release-notes\n",
      "\n",
      "  BigQuery\n",
      "      o Added --ignore_unknown_values flag to bq mkdef command.\n",
      "      o Added support for BigQuery BI Engine reservations in bq cli.\n",
      "\n",
      "  Cloud Datastore Emulator\n",
      "      o Release Cloud Datastore Emulator version 2.1.0\n",
      "        * Implement export/import for emulator. For details, refer to\n",
      "          <https://cloud.google.com/datastore/docs/tools/emulator-export-import>.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed the display of error codes in gcloud sql operations list.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted gcloud compute instance-groups managed update to beta\n",
      "        together with --health-check, --initial-delay and --clear-autohealing\n",
      "        flags.\n",
      "      o Promoted --hostname flag of gcloud compute instances create to GA.\n",
      "      o Added --physical-block-size flag to gcloud beta compute disks create.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Added --num-flaky-test-attempts flag to gcloud beta firebase test\n",
      "        android run and gcloud beta firebase test ios run to rerun failed\n",
      "        executions multiple times.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --security-group flag of gcloud container clusters\n",
      "        create to\n",
      "    beta. The flag enables support for Google Groups in Kubernetes RBAC rules.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "230.0.0 (2019-01-15)\n",
      "  Breaking Changes\n",
      "      o **(App Engine)** Fixed a bug where symlinked directories were skipped\n",
      "        on source upload. Second Generation runtimes and source directories\n",
      "        using .gcloudignore now upload the contents of symlinked directories,\n",
      "        matching the behavior of First Generation runtimes. To explicitly skip\n",
      "        a symlinked directory, add it to .gcloudignore.\n",
      "      o **(Cloud Functions)** Fixed a bug where symlinked directories were\n",
      "        skipped on source upload. To explicitly skip a symlinked directory, add\n",
      "        it to .gcloudignore.\n",
      "      o **(Cloud SQL)** Made the flags --region, --gce-zone, and --zone\n",
      "        mutually exclusive for the command gcloud sql instances create.\n",
      "      o **(Cloud SQL)** Deprecated the creation of First Generation Cloud SQL\n",
      "        instances, adding a warning and confirmation prompt to gcloud sql\n",
      "        instances create.\n",
      "\n",
      "  Cloud Build\n",
      "      o Released cloud-build-local v0.5.0; see release notes:\n",
      "        <https://github.com/GoogleCloudPlatform/cloud-build-local/releases/tag/v0.5.0>.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Added the flag --zone to gcloud sql instances create as an\n",
      "        alternative to --gce-zone, which is now deprecated.\n",
      "      o Deprecated First Generation Cloud SQL instances, adding warnings to\n",
      "        gcloud sql instances <describe|patch>.\n",
      "\n",
      "  Cloud Scheduler\n",
      "      o Added support for all of App Engine's regions to Cloud Scheduler.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        GA in the following command groups:\n",
      "        * gcloud compute instances\n",
      "        * gcloud compute sole-tenancy node-templates\n",
      "        * gcloud compute sole-tenancy node-groups\n",
      "      o Promoted --boot flag of gcloud compute instances attach-disk to GA.\n",
      "      o Deprecated --auto-create-routes flag of gcloud alpha compute networks\n",
      "        peerings create in Beta.\n",
      "      o Promoted gcloud compute networks peerings update command to Beta.\n",
      "      o Promoted import-custom-routes and export-custom-routes flags to Beta\n",
      "        in gcloud compute networks peerings create command.\n",
      "      o Deprecated and renamed the following --shielded-vm- flags:\n",
      "        * --shielded-vm-secure-boot as --shielded-secure-boot\n",
      "        * --shielded-vm-vtpm as --shielded-vtpm\n",
      "        * --shielded-vm-integrity-monitoring as\n",
      "          --shielded-integrity-monitoring\n",
      "        * --shielded-vm-learn-integrity-policy as\n",
      "          --shielded-learn-integrity-policy\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Deprecated the --app-package and --test-package flags in gcloud\n",
      "        firebase test android run commands; the application and test package\n",
      "        names will be parsed from the APK manifest by default.\n",
      "      o Removed three robo test args that were deprecated 6+ months ago:\n",
      "        --max-steps, --max-depth, and --app-initial-activity.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "#============================================================#\n",
      "#= Creating update staging area                             =#\n",
      "#============================================================#\n",
      "#= Uninstalling: BigQuery Command Line Tool                 =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud SDK Core Libraries                   =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud Storage Command Line Tool            =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Alpha Commands                      =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Beta Commands                       =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud cli dependencies                    =#\n",
      "#============================================================#\n",
      "#= Installing: BigQuery Command Line Tool                   =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud SDK Core Libraries                     =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud Storage Command Line Tool              =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Alpha Commands                        =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Beta Commands                         =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud cli dependencies                      =#\n",
      "#============================================================#\n",
      "#= Creating backup and activating new installation          =#\n",
      "#============================================================#\n",
      "\n",
      "Performing post processing steps...\n",
      "...........................................done.\n",
      "\n",
      "Update done!\n",
      "\n",
      "To revert your SDK to the previously installed version, you may run:\n",
      "  $ gcloud components update --version 229.0.0\n",
      "\n",
      "\n",
      "All components are up to date.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud components update --quiet\n",
    "gcloud components install alpha --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"poetry\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model/export/Servo | tail -1)\n",
    "gcloud alpha ml-engine versions create --machine-type=mls1-highcpu-4 ${MODEL_VERSION} \\\n",
    "       --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=1.5 --config=mlengine.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kubeflow\n",
    "\n",
    "Follow these instructions:\n",
    "* On the GCP console, launch a Google Kubernetes Engine (GKE) cluster named 'poetry' with 2 nodes, each of which is a n1-standard-2 (2 vCPUs, 7.5 GB memory) VM\n",
    "* On the GCP console, click on the Connect button for your cluster, and choose the CloudShell option\n",
    "* In CloudShell, run: \n",
    "    ```\n",
    "    git clone https://github.com/GoogleCloudPlatform/training-data-analyst`\n",
    "    cd training-data-analyst/courses/machine_learning/deepdive/09_sequence\n",
    "    ```\n",
    "* Look at [`./setup_kubeflow.sh`](setup_kubeflow.sh) and modify as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AppEngine\n",
    "\n",
    "What's deployed in Cloud ML Engine or Kubeflow is only the TensorFlow model. We still need a preprocessing service. That is done using AppEngine.  Edit application/app.yaml appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: python\r\n",
      "env: flex\r\n",
      "entrypoint: gunicorn -b :$PORT main:app\r\n",
      "service: mlpoetry\r\n",
      "\r\n",
      "handlers:\r\n",
      "- url: /\r\n",
      "  script: main.app\r\n",
      "- url: /.*\r\n",
      "  script: main.app\r\n",
      "\r\n",
      "env_variables:\r\n",
      "  MODEL_NAME: poetry\r\n",
      "  PROJECT_ID: cloud-training-demos\r\n",
      "  VERSION_NAME: v1\r\n",
      "  PROBLEM_NAME: poetry_line_problem\r\n",
      "  T2T_USR_DIR: instance/poetry/trainer\r\n",
      "  HPARAMS: transformer_poetry\r\n",
      "  DATADIR: gs://cloud-training-demos-ml/poetry/data\r\n"
     ]
    }
   ],
   "source": [
    "!cat application/app.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd application\n",
    "#gcloud app create  # if this is your first app\n",
    "#gcloud app deploy --quiet --stop-previous-version app.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visit https://mlpoetry-dot-cloud-training-demos.appspot.com and try out the prediction app!\n",
    "\n",
    "<img src=\"diagrams/poetry_app.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2018 Google Inc. Licensed under the Apache License, Version 2.0 (the \\\"License\\\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
